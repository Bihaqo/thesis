\chapter*{Введение}							% Заголовок
\addcontentsline{toc}{chapter}{Введение}	% Добавляем его в оглавление

\section{Актуальность темы исследования} \label{sec:intro-importance}
Машинное обучение переживает период бурного развития и за последние годы было решено много важных на практике проблем, таких как распознавание объектов изображенных на фотографиях, сегментация фотографий, машинный перевод, распознфвание речи, и др. Тем не менее, алгоритмы показывающие наилучшее качество во многих задачах (такие как нейронные сети и Марковские Случайные Поля) вычислительно затратны как во время обучения, так и во время использования моделей. Так, для обучения модели машинного перевода может требоваться месяцы времени на вычислительном кластере~\cite{}, а итоговая модель может занимать сотни мегабайт и требовать \alert{ФЛОПС?} для обработки одного предложения, что оказывается слишком затратно для использования на мобильных и встроенных устройствах, как по части быстродействия и требования к памяти, так и по части использования ресурса батарейки. В связи с этим возникает потребность к разработке новых более быстрых методов обучения моделей, способов сжатия и ускорения моделей, и разработки новых менее ресурсо-затратных моделей с нуля. По всем этим направлениям ведуться активные исследования в лучших лабораториях машинного обучения в мире.

В данной диссертации исследуется возможность применения алгоритмов малорангового сжатия тензоров для решения озвученных выше задач. 
\section{Историческая справка} \label{sec:intro-history}
Про развитие МЛ, про тензорные методы в МЛ
\section{Общая характеристика кандидатской работы} \label{sec:intro-overview}
\subsection{Научная новизна} \label{sec:intro-novelity}

\subsection{Практическая ценность} \label{sec:intro-appliability}
\subsection{Положения, выносимые на защиту} \label{sec:intro-defended-topics}
\begin{enumerate}
	\item Комплекс программ для работы с разложением в тензорный поезд на языке Python с поддержкой графических ускорителей;
	\item Вычислительный метод оценки нормировочной константы марковского случайного поля;
	\item Вычислительный метод оценки поиска собственного разложения матриц в ТТ-формате в приложении к поиску конфигурации минимума энергии Марковского случайного поля;
	\item Модель Искусственной Нейронной сети опережающая аналоги по соотношению число настраиваемых параметров к качеству работы.
	\item Модель Рекуррентной Искусственной Нейронной Сети для которой возможно быстрое обучение с помощью Римановой оптимизации.
\end{enumerate}
\subsection{Апробация работы и публикации} \label{sec:intro-publications}
\subsection{Личный вклад} \label{sec:intro-publications}
\section{Содержание работы по главам} \label{sec:intro-detailed-table-of-contents}
\section{Благодарности} \label{sec:intro-acknowledgment}

\newcommand{\actuality}{}
\newcommand{\progress}{}
\newcommand{\aim}{{\textbf\aimTXT}}
\newcommand{\tasks}{\textbf{\tasksTXT}}
\newcommand{\novelty}{\textbf{\noveltyTXT}}
\newcommand{\influence}{\textbf{\influenceTXT}}
\newcommand{\methods}{\textbf{\methodsTXT}}
\newcommand{\defpositions}{\textbf{\defpositionsTXT}}
\newcommand{\reliability}{\textbf{\reliabilityTXT}}
\newcommand{\probation}{\textbf{\probationTXT}}
\newcommand{\contribution}{\textbf{\contributionTXT}}
\newcommand{\publications}{\textbf{\publicationsTXT}}

\input{common/characteristic} % Характеристика работы по структуре во введении и в автореферате не отличается (ГОСТ Р 7.0.11, пункты 5.3.1 и 9.2.1), потому её загружаем из одного и того же внешнего файла, предварительно задав форму выделения некоторым параметрам

% \textbf{Объем и структура работы.} Диссертация состоит из~введения, четырёх глав, заключения и~двух приложений.
%% на случай ошибок оставляю исходный кусок на месте, закомментированным
%Полный объём диссертации составляет  \ref*{TotPages}~страницу с~\totalfigures{}~рисунками и~\totaltables{}~таблицами. Список литературы содержит \total{citenum}~наименований.
%
%
%
%Matrix and tensor decompositions were recently used to speed up the inference time of convolutional neural networks~\cite{Denil2014speedup,lebedev2014speeding}. While we focus on fully-connected layers, \cite{lebedev2014speeding} used the CP-decomposition to compress a $4$-dimensional convolution kernel and then used the properties of the decomposition to speed up the inference time. Their work share the same spirit with our method and the approaches can be readily combined.

Полный объём диссертации составляет
\formbytotal{TotPages}{страниц}{у}{ы}{}, включая
\formbytotal{totalcount@figure}{рисун}{ок}{ка}{ков} и
\formbytotal{totalcount@table}{таблиц}{у}{ы}{}.   Список литературы содержит  
\formbytotal{citenum}{наименован}{ие}{ия}{ий}.
