\chapter*{Введение}							% Заголовок
\addcontentsline{toc}{chapter}{Введение}	% Добавляем его в 

\section{Актуальность темы исследования} \label{sec:intro-importance}
Машинное обучение переживает период бурного развития и за последние годы было решено много важных на практике проблем, таких как распознавание объектов изображенных на фотографиях, сегментация фотографий, машинный перевод, распознование речи, и др. Тем не менее, алгоритмы показывающие наилучшее качество во многих задачах (такие как нейронные сети и марковские случайные поля) вычислительно затратны как во время обучения, так и во время использования моделей. Так, для обучения модели машинного перевода может требоваться месяцы времени на вычислительном кластере~\cite{}, а итоговая модель может занимать сотни мегабайт и требовать \alert{ФЛОПС?} для обработки одного предложения, что оказывается слишком затратно для использования на мобильных и встроенных устройствах, как по части быстродействия и требования к памяти, так и по части использования ресурса аккумулятора. В связи с этим возникает потребность к разработке новых более быстрых методов обучения моделей, способов сжатия и ускорения моделей, и разработки новых менее ресурсо-затратных моделей с нуля. По всем этим направлениям ведутся активные исследования в лабораториях машинного обучения в мире.

В данной диссертации исследуется возможность применения алгоритмов малорангового сжатия тензоров для решения озвученных выше задач. 
%\section{Историческая справка} \label{sec:intro-history}
%Про развитие МЛ, про тензорные методы в МЛ
%\section{Общая характеристика кандидатской работы} \label{sec:intro-overview}
\subsection{Цель диссертационной работы}
Первой целью настоящей диссертационной работы является разработка новых моделей машинного обучения основанных на аппарате тензорных разложений не уступающих аналогам по качеству, но превосходящих аналоги по скорости работы и (или) компактности, а так же в разработке новых методах обучения и использования существующих моделей опережающих аналоги по скорости работы. Второй целью работы является разработка программного комплекса упрощающего дальнейшие исследования на стыке машинного обучения и тензорных методов.

\subsection{Научная новизна} \label{sec:intro-novelity}
Предложен новый метод оценки нормировочной константы и маргинальных распределений марковского случайного поля (статистики, использующиеся как для обучения марковских случайных полей, так и для их применения к новым объектам), существенно превсходящий аналоги по скорости работы. Получены теоретические результаты по оценке точности работы предложенных методов. \alert{(пока не описано). Предложены аналитические формулы для перевода потенциалов часто используемых в марковских случайных полях в низкоранговый тензорный формат.}

Предложена модель искусственной нейронной сети на порядки превосходящая аналоги по компактности представления (количества настраиваемых параметров при сравнимом качестве работы). Предложена модель рекуррентный нейросети допускающая обучение с помощью стохастической римановой оптимизации, что приводит к более быстрому обучению модели по сравнению с обучением классическими методами, такими как стохастический градиентный спуск примененный к параметрам. Разработанный метод римановой оптимизации использует структуру модели и оптимизируемой функции чтобы достичь высокой скорости работы.

\subsection{Практическая ценность} \label{sec:intro-appliability}
Предложенные методы можно использовать для ускорения обучения и применения к новым объектам марковских случайных полей, для сжатия нейросетей для их применения в мобильных и встроенных устройствах и для более более быстрого обучения рекурентных нейросетей. Разработан программный комплекс для работы с разложением в Тензорный Поезд, обладающий такими необходимыми для использования в области машинного обучения функции, как поддержка графических ускорителей, автоматическое дифференцирование, параллельная работа с множеством обучающих объектов, и богатая поддержка римановой оптимизации.

\subsection{Положения, выносимые на защиту} \label{sec:intro-defended-topics}
\begin{enumerate}
	\item Разработан комплекс программ для работы с разложением в Тензорный Поезд на языке Python с поддержкой графических ускорителей;
	\item Предложен вычислительный метод оценки нормировочной константы и маргинальных распределений марковского случайного поля;
	\item Получена теоретическая оценка точности работы предложенного метода;
%	\item Вычислительный метод оценки поиска собственного разложения матриц в ТТ-формате в приложении к поиску конфигурации минимума энергии Марковского случайного поля;
	\item Предложена модель искусственной нейронной сети опережающая аналоги по соотношению число настраиваемых параметров к качеству работы.
	\item Предложена модель рекуррентной искусственной нейронной сети для которой возможно быстрое обучение с помощью Римановой оптимизации.
\end{enumerate}
\subsection{Апробация работы} \label{sec:intro-talks}
Результаты диссертационной работы докладывались автором и обсуждались на следующих научных конференциях и семинарах
\begin{enumerate}
	\item International Conference on Learning Representations workshop, 2017, Тулон, Франция
	\item Neural Information Processing Systems, 2015, Торронто, Канада
	\item Tensor Methods in Machine Learning, 2015, Нью Йорк, США
	\item International conference on Matrix Methods in Mathematics and Applications, 2015, Москва, Россия
	\item Google research seminar, 2014, Маунтин-Вью, США
	\item SIAM Conference on Imaging Science, 2014, Гонконг
	\item International Conference on Machine Learning, 2014, Пекин, Китай
	\item \alert{добавить семинар ИВМ?}
\end{enumerate}
\subsection{Публикации} \label{sec:intro-publications}
Основные результаты данной диссертационной работы опубликованы в следующих работах.
Работы, опубликованные в изданиях, входящих в перечень рецензируе-мых научных изданий, индексируемых Web of Science
\begin{enumerate}
	\item Novikov A., Podoprikhin D., Osokin A., Vetrov D., Tensorizing neural networks. In Advances in Neural Information Processing Systems 28 (NIPS), 2015.
	\item Novikov A., Rodomanov A., Osokin A., Vetrov D., Putting MRFs on a Tensor Train. In International Conference on Machine Learning (ICML), 2014.
\end{enumerate}
Работы, опубликованные в прочих изданиях
\begin{enumerate}
	\item Novikov A., Trofimov M., Oseledets I. "Tensor Train decomposition on TensorFlow (T3F)." arXiv preprint arXiv:1801.01928 (2018).
	\item Novikov A., Trofimov M., Oseledets I. "Exponential machines." arXiv preprint arXiv:1605.03795 (2016).
	\item Новиков А., Родоманов А., Осокин А., Ветров Д., Тензорный поезд в марковском случайном поле, журнал «Интеллектуальные системы. Терия и приложения», pp. 293–318, 2014.
\end{enumerate}

\subsection{Личный вклад} \label{sec:intro-publications}
\section{Содержание работы по главам} \label{sec:intro-detailed-table-of-contents}
\section{Благодарности} \label{sec:intro-acknowledgment}

\newcommand{\actuality}{}
\newcommand{\progress}{}
\newcommand{\aim}{{\textbf\aimTXT}}
\newcommand{\tasks}{\textbf{\tasksTXT}}
\newcommand{\novelty}{\textbf{\noveltyTXT}}
\newcommand{\influence}{\textbf{\influenceTXT}}
\newcommand{\methods}{\textbf{\methodsTXT}}
\newcommand{\defpositions}{\textbf{\defpositionsTXT}}
\newcommand{\reliability}{\textbf{\reliabilityTXT}}
\newcommand{\probation}{\textbf{\probationTXT}}
\newcommand{\contribution}{\textbf{\contributionTXT}}
\newcommand{\publications}{\textbf{\publicationsTXT}}

\input{common/characteristic} % Характеристика работы по структуре во введении и в автореферате не отличается (ГОСТ Р 7.0.11, пункты 5.3.1 и 9.2.1), потому её загружаем из одного и того же внешнего файла, предварительно задав форму выделения некоторым параметрам

% \textbf{Объем и структура работы.} Диссертация состоит из~введения, четырёх глав, заключения и~двух приложений.
%% на случай ошибок оставляю исходный кусок на месте, закомментированным
%Полный объём диссертации составляет  \ref*{TotPages}~страницу с~\totalfigures{}~рисунками и~\totaltables{}~таблицами. Список литературы содержит \total{citenum}~наименований.
%
%
%
%Matrix and tensor decompositions were recently used to speed up the inference time of convolutional neural networks~\cite{Denil2014speedup,lebedev2014speeding}. While we focus on fully-connected layers, \cite{lebedev2014speeding} used the CP-decomposition to compress a $4$-dimensional convolution kernel and then used the properties of the decomposition to speed up the inference time. Their work share the same spirit with our method and the approaches can be readily combined.

Полный объём диссертации составляет
\formbytotal{TotPages}{страниц}{у}{ы}{}, включая
\formbytotal{totalcount@figure}{рисун}{ок}{ка}{ков} и
\formbytotal{totalcount@table}{таблиц}{у}{ы}{}.   Список литературы содержит  
\formbytotal{citenum}{наименован}{ие}{ия}{ий}.
