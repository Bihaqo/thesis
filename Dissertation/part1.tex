% \cite[с.~54]{Sokolov}
\chapter{Основные понятия} \label{chap:definitions}
Данный раздел является вводным и в нем дается справках по основным понятиям использующемся в диссертации: разложению в Тензорный Поезд и машинному обучению (распознаванию образов).
\section{Разложение в тензорный поезд} \label{sec:tt-decomposition}

\begin{figure}[ht]
  \begin{center}
    \def\svgwidth{14cm}
    \normalsize
    \includesvg{images/TT}
    \caption{Иллюстрация разложения в Тензорный Поезд для тензора $\tens{A}$ размера $3 \times 4 \times 4 \times 3$ все ТТ-ранги которого равны $3$. \label{fig:TT}}
    \end{center}
\end{figure}

Будем говорить, что $d$-мерный тензор~$\tens{A} \in \mathcal{R}^{n^d}$ представлен в TT\hyp{}формате, если для всех размерностей~$k=1,\ldots,d$ и всех значений индексов по этим размерностям~$i_k = 1, \ldots, n_k$ ($n = \max_{k=1,\ldots,d} n_k$) существуют матрицы~$G_k^{\tens{A}}[i_k]$, такие, что каждый элемент тензора~$\tens{A}$ представим в виде произведения этих матриц (см. Рис.~\ref{fig:TT}):
\begin{equation}
\label{TT-format}
\tensel{A}_{i_1, \ldots, i_d} = G_1^{\tens{A}}[i_1] G^{\tens{A}}_2[i_2] \dotsm G^{\tens{A}}_d[i_d].
\end{equation}
При этом все матрицы относящиеся к одной и той же размерности $k$ должны иметь одинаковые размеры~$\rank_{k-1}(\tens{A}) \times \rank_k(\tens{A})$. Чтобы результат матричного произведения~\eqref{TT-format} был равен числу (соответствующему элементу тензора~$\tens{A}$), положим~$\rank_{0}(\tens{A}) = \rank_{d}(\tens{A}) = 1$. Последовательность~$\left\{\rank_k(\tens{A})\right\}_{k=0}^d$ будем называть \emph{TT\hyp{}рангами}~тензора $\tens{A}$, а максимальный элемент последовательности~-- \emph{максимальным TT\hyp{}рангом} тензора~$\tens{A}$:~$\rank(\tens{A}) = \max_{k = 0, \dots, d} \rank_k(\tens{A})$.
Набор матриц~$G^{\tens{A}}_k$, соответствующих одному измерению, называется \emph{TT\hyp{}ядром} тензора~$\tens{A}$.
Представление тензора в TT\hyp{}формате будем называть \emph{TT\hyp{}разложением} или \emph{TT\hyp{}представлением}.

Для любого $d$-мерного тензора~$\tens{A}$ существует TT\hyp{}представление с максимальным TT\hyp{}рангом $\rank(\tens{A}) \leq n^{\frac{d}{2}}$~(см. теорему~2.1 Оселедца \cite{oseledets2011ttMain}). Отметим, что TT\hyp{}представление тензора не единственно.


Для обозначения~$(\alpha_{k-1}, \alpha_{k})$-ого элемента матрицы~$G_k^{\tens{A}}[i_k]$ будет использоваться символ~$G_k^{\tens{A}}[i_k](\alpha_{k-1}, \alpha_{k})$. При этом при реализации ТТ-разложения на ЭВМ, ядра $G_k^{\tens{A}}$ будут храниться как трехмерные массивы индексируемые в порядке $\alpha_{k-1}, i_k, \alpha_{k}$, так как такой порядок индексов допускает более эффективную реализацию некоторых операций. 
Пользуясь определением произведения матриц, можно переписать формулу~\eqref{TT-format} через элементы TT\hyp{}ядер:

\begin{equation}
	\label{TT-format-sum}
	\tens{A}_{i_1, \ldots, i_d} =
	\sum_{\alpha_0, \ldots, \alpha_d} G^{\tens{A}}_1[i_1](\alpha_0, \alpha_1) \ldots G^{\tens{A}}_d[i_d](\alpha_{d-1}, \alpha_n).
\end{equation}

Для краткости, в случае когда это не вызывает разночтений, тензор ТТ-разложение которого сейчас рассматривается может опускаться в обозначениях: $G_k$ вместо $G_k^{\tens{A}}$, $\rank_{k-1}$ вместо $\rank_{k-1}(\tens{A})$, и т.п.

Для хранения всех элементов тензора~$\tens{A}$ требуется~$\prod_{k=1}^d n_k$ ячеек памяти, тогда как хранение~$\tens{A}$ в TT\hyp{}формате требует~$\sum_{k=1}^d n_k \rank_{k-1}(\tens{A}) \rank_{k}(\tens{A})$. Таким образом, TT\hyp{}представление тензора с низкими TT\hyp{}рангами существенно компактнее перечисления его элементов.

Существуют две различные постановки задачи перевода тензора в TT\hyp{}формат: точное TT\hyp{}представление (алгоритм TT-SVD~\cite{oseledets2011ttMain}, применимый для небольших тензоров), и построение приближенного TT\hyp{}представления по небольшому подмножеству элементов тензора. Наилучшим из алгоритмов второго класса в настоящее время является метод AMEn-cross~\cite{dolgov2013amenCross}.

\begin{table}[t]
\caption{Операции, которые можно эффективно выполнять над тензорами в TT\hyp{}формате. Для каждой операции указана её вычислительная сложность и TT\hyp{}ранг результата для ситуаций, когда результат является тензором в TT\hyp{}формате.}
\label{TT-tensor-operations}
\vskip 0.05in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{l@{\;\;}l@{\;\;}l}
\hline
Операция & Ранг результата & Вычислительная сложность \\
\hline
% \abovespace
$\tens{C} = \tens{A} \cdot \const$      & $\rank(\tens{C}) = \rank(\tens{A})$             & $O(d  \rank(\tens{A}))$\\
$\tens{C} = \tens{A} + \const$          & $\rank(\tens{C}) = \rank(\tens{A})\!+\!1$         & $O(n  d  \rank^2(\tens{A}))$\\
$\tens{C} = \tens{A} + \tens{B}$           & $\rank(\tens{C}) \leq \rank(\tens{A})\!+\!\rank(\tens{B})$       & $O(n  d \, (\rank(\tens{A}) + \rank(\tens{B}))^2)$\\
$\tens{C} = \tens{A} \odot \tens{B}$       & $\rank(\tens{C}) \leq \rank(\tens{A})  \rank(\tens{B})$   & $O(n  d  \rank^2(\tens{A})  \rank^2(\tens{B}))$\\
$\vec{c}=M\vec{b}$                          & $\rank(\vec{c}) \leq \rank(M) \rank(\vec{b})$       & $O(n d^2 \rank^2(M) \rank^2(\vec{b}))$\\
%\belowspace
$\sumall \tens{A}$                & --                      & $O(n  d  \rank^2(\tens{A}))$\\
$\left \| \tens{A} \right \|_F$       & --                      & $O(n  d  \rank^3(\tens{A}))$\\
% \belowspace
$\tens{C} = \round(\tens{A}, \varepsilon)$ & $\rank(\tens{C}) \leq \rank(\tens{A})$          & $O(n d \rank^3(\tens{A}))$\\
\hline
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Одним из основных достоинств TT\hyp{}формата является возможность эффективно применять различные операции к тензорам в TT\hyp{}формате:
умножение тензора на константу, добавление константы к тензору, поточечное сложение и умножение тензоров (результат этих операций~--- это тензор в TT\hyp{}формате с возросшими TT\hyp{}рангами); подсчет глобальных характеристик тензора, таких как сумма всех элементов или норма Фробениуса. Обзор часто-употребимых операций над ТТ-тензорами приведен в таблице~\ref{TT-tensor-operations} (детальный обзор см в работе~\cite{oseledets2011ttMain}).

Примение операций к TT\hyp{}тензорам увеличивает TT\hyp{}ранги даже в том случае, когда существует низкоранговое TT\hyp{}представление результата. Чтобы контролировать рост TT\hyp{}рангов существует операция TT\hyp{}округления.
По TT\hyp{}представлению тензора~$\tens{A}$ и относительной точности~$\varepsilon \geq 0$ операция TT\hyp{}округления $\round(\tens{A},\varepsilon)$ найдет тензор~$\widehat{\tens{A}}$ в TT\hyp{}формате который, во-первых, достаточно близок к тензору~$\tens{A}$: $\| \tens{A} - \tens{\widehat{A}} \|_F \leq \varepsilon  \| \tens{A}  \|_F$ и, во-вторых, обладает минимальными TT\hyp{}рангами среди всех тензоров~$\tens{B}: \| \tens{A} - \tens{B} \|_F \leq \frac{\varepsilon}{\sqrt{n - 1}}  \| \tens{A}  \|_F$. Наличие операции TT\hyp{}округления позволяет применять последовательность операций к тензорам (например, округляя результат после применения каждой операции), контролируя рост TT\hyp{}рангов.


Для повышения эффективности работы с векторами и матрицами специальным образом вводятся понятия TT\hyp{}формата вектора и ТТ\hyp{}формата матрицы.
Пусть существует отображение $f$ между индексами вектора~$\vec{b}\in\mathbb{R}^{n^d}$ и $d$-мерными векторами $\vec{j} = (j_1, \dots, j_d)$.%\footnote{
%Количество элементов в векторе~$\vec{b}$ равно $\prod_{i = 1}^n d_i$.
%}.
\emph{TT\hyp{}представлением вектора~$\vec{b}$} называется TT\hyp{}представление тензора~$\tens{B}$, содержащего все элементы~$\vec{b}$: $\tensel{B}_{j_1, \dots, j_d} = \vec{b}_{f^-1(j_1, \dots, j_d)}$.


Определим понятие TT\hyp{}формата для матриц. Пусть существует отображение между индексами строк и столбцов матрицы~$\mat{M}$ в $d$-мерные вектора $\vec{i}$ и $\vec{j}$ соответственно. Переупорядочим размерности и представим получившийся d-мерный тензор~$\tens{M}$ в TT\hyp{}формате:
\begin{equation*}
\tensel{M}((i_1, j_1),\ldots,(i_d,j_d))=G_1^{\tens{M}}[i_1,k_1]\dots G_d^{\tens{M}}[i_d, j_d],
\end{equation*}
где $G_k$, $k = 1,\dots,n$ --- это TT\hyp{}ядра, а~$G_k^{\tens{M}}[i_k,j_k]$ -- матрицы для любого значения индексов $i_k = 1, \ldots, m_k$, $j_k = 1, \ldots, n_k$. \emph{TT\hyp{}представлением матрицы~$\mat{M}$} будем называть TT\hyp{}представление тензора~$\tens{M}$. Отметим, что матрица в TT\hyp{}формате не обязана быть квадратной, т.~к. $i_k$ и $j_k$ могут принимать разное число возможных значений.


Для матрицы~$M$ и вектора~$\vec{b}$ представленных в TT\hyp{}формате можно эффективно вычислять произведение $\vec{c}=M\vec{b}$ (если соответствующие размерности совпадают). Результатом этой операции является вектор~$\vec{c}$ в TT\hyp{}формате с рангами равными произведению рангов~$M$ и $\vec{b}$: $\rank_i(\vec{c}) = \rank_i(M) \rank_i(\vec{b})$.

Наличие специального определения TT\hyp{}формата для векторов и матриц позволяет применять операции линейной алгебры к задачам большого размера. Например, для поиска минимального элемента тензора можно вытянуть его в диагональную матрицу и применить приближенный метод поиска минимальных собственных значений.

\section{Машинное обучение (распознование образов)} \label{sec:ml}
Данный раздел является вводным и в нем приведен краткий обзор методов машинного обучения и их приложений на практике.

%\subsection{Постановка задачи обучения с учителем}
Пусть задана выборка независимых одинаково распределенных обучающих пар $\{(\vec{x}^{(f)}, y^{(f)})\}_{f=1}^N \sim p(x, y)$ где объект номер~$f$ описывается вектором $\vec{x}^{(f)}$ длины $d$, а так же каждому объекту сопоставляется целевая переменная~$y^{(f)}$. Пусть так же задана функция потерь $\ell(\widehat{y}, y):\mathbb{R}^2 \to \mathbb{R}$ отображающая предсказанную целевую переменную~$\widehat{y}$ и правильный ответ~$y$ взятый из обучающей выборки в вещественное значение означающее ошибку допущенную при предсказании. Цель задачи обучения с учителем состоит в том, чтобы найти функцию $f$ отображающую признаковые описания $\vec{x}$ в предсказания $y$ приближающую минимум средней функции потерь на заданном распределении:
\begin{equation}
\label{eq:risk-minimization}
f \approx \argmin_{g(x)} \mathbb{E}_{p(x, y)} \ell(g(x), y)	
\end{equation}
Так как в постановки задачи не дается распределение $p(x, y)$, а только конечная обучающая выборка $\{(\vec{x}^{(f)}, y^{(f)})\}_{f=1}^N$, то задачу~\eqref{eq:risk-minimization} заменяют на задачу \emph{минимизации эмпирического риска} по некому подклассу функций~$\mathcal{F}$
\begin{equation}
\label{eq:emprical-risk-minimization}
f \approx \argmin_{g(x) \in \mathcal{F}} \frac{1}{N} \sum_{f=1}^N \ell(g(x^{(f)}), y^{(f)})
\end{equation}
Необходимость введения подкласса функций~$\mathcal{F}$ обусловлена \emph{проблемой переобучения}: множество глобальных минимизаторов эмпирического риска содержит функции запоминающие выборку, то есть на объектах обучающей выборки возвращающие правильные ответы, а на всех остальных (новых объектах) -- случайные. На практике, класс функций~$\mathcal{F}$ выбирают так чтобы он не содержал такие нежелательные решения.

Простейшим примером постановки задачи обучения с учителем является линейная регрессия, которая получается при выборе класса линейных функций~$\mathcal{F}$ и квадратичной функции потерь~$\ell$: 
\begin{equation}
\label{eq:linear-model}
\mathcal{F} = \{\widehat{y}_{\text{linear}}(\vec{x}) = \langle \vec{x}, \vec{w} \rangle + b \mid \vec{w} \in \mathcal{R}^d, b \in \mathcal{R}\}
\end{equation}
%\begin{equation}
%\label{eq:linear-model}
\begin{equation}
\label{eq:linear-loss}
\min_{\vec{w}, b}\sum_{f=1}^N \left(\langle \vec{x}^{(f)}, \vec{w} \rangle + b - y^{(f)}\right)^2% + \frac{\lambda}{2} \norm{\vec{w}}^2_2,
\end{equation}
%\end{equation}

Если значения целевой переменной $y$ принадлежат конечному неупорядоченному множеству (пусть, не ограничивая общности, $y \in \{1, \ldots, K\}$), то такую задачу машинного обучения называют \emph{задачей классификации}. Простейшим примером постановки задачи классификации является логистическая регрессия, которая получается при рассмотрении класса функций вида `нормированная экспонента от линейной функции', и функции потерь вида кросс-энтропии
\begin{equation}
%\label{eq:linear-loss}
\min_{\vec{w}_1, \ldots, \vec{w}_K, b_1, \ldots, b_K} - \sum_{f=1}^N \left (\langle \vec{x}^{(f)}, \vec{w}_{y^{(f)}} \rangle + b_{y^{(f)}} - \log \sum_{j=1}^K \exp\left(\langle \vec{x}^{(f)}, \vec{w}_j \rangle + b_j\right ) \right )% + \frac{\lambda}{2} \norm{\vec{w}}^2_2,
\end{equation}

На практике, в задачах классификации и регрессии вместо линейных моделей часто используют \emph{исскуственные нейронные сети}. Исскуственными нейронными сетями принято называть композицию некоторого класса базовых функций, которые являются непрерывными и (почти всюду) дифференцируемыми. Чаще всего в качестве базового класса функций рассматривают линейные отображения и поэлементное применение функции сигмоиды $\sigma(x) = \frac{1}{1 + \exp(x)}$ или функции ReLU $f(x) = \max\{0, x\}$. Для работы с визуальными данными (когда $\vec{x}$ это изображение) так же часто применяют операцию свертки изображения с ядром, значения которого являются настраиваемыми параметрами. Примером нейронной сети может являться следующий класс функций заданный с точностью до настраиваемых параметров $\mat{W}_1, \mat{W}_2, \vec{w}_3, \vec{b}_1, \vec{b}_2, b_3$
\[
f(\vec{x}) = \langle \vec{w}_3, \sigma(\mat{W}_2 \sigma(\mat{W}_1 \vec{x} + \vec{b}_1) + \vec{b}_2) \rangle + b_3
\]


