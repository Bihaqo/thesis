\chapter{Нейронные сети} \label{chap:tensornet}
Искусственные нейронные сети опережают другие подходы во многих задачах распознавания образов, распознавания речи, перевода, и т.п. [CITE TODO]
Причиной успеха этого класса моделей является их большая гибкость и доступность большого количества вычислительных ресурсов (таких как кластеры графических ускорителей GPU).
И, хотя искусственные нейросети успешно применяются в промышленности (от машинного перевода, ...), их использования в мобильных и встроенных устройствах без доступа к интернету остается ограниченным.

Причина этому -- высокие требования к доступной памяти, вычислительной мощности и использованию аккумулятора. Например, популярная на сегодняшний день модель VGG-16 потребляет [TODO].

В связи с этим многие работы были посвящены задачам сжатия и ускорения нейронных сетей. Обратите внимание, что размер требуемой памяти является одним из ограничивающих факторов к снижению энергопотребления моделей. Так, при использовании 45 нм техпроцесса по технологии КМОП, требуется 0.9 пДж чтобы сложить два 32-битных числа с плавающей точкой, доступ к одному 32-битному числу в кэше SRAM требует 5 пДж, и доступ к 32-битному числу в оперативной памяти требует 640 пДж, что на три порядка больше по сравнению с операцией сложения. Большие нейросети не помещаются в кэш процессора и таким образом требует ресурсоемкого доступа к оперативной памяти.

В данной главе представлен тензорный подход к сжатию и ускорению (а значит и снижению энергопотребления) нейронных сетей.

\section{Обзор существующих подходов к сжатию нейронных сетей} \label{sec:tensornet-alternatives}
В современных нейронных сетях до 99\% памяти занимают веса полносвязных слоев. В связи с этим, многие работы ставили своей целью исследование возможности сжатия полносвязных слоев. Одним из первых рассмотренных подходов к сжатию стало ограничение ранга матрицы полносвязного слоя~\cite{Denil2013predicting}. Ограничение случайного подмножества элементов матрицы так, чтобы все элементы каждого подмножества имели одно и то же значение~\cite{chen2015compressing} позволило сжимать полносвязные слои в 8 раз без потери качества. Так же рассматривалась более общая постановка, в которой ограничивалось число возможных значений которые могут принимать различные элементы матрицы~\cite{gong2014PQcompressing}. Было показано, что использование 16-битный чисел с плавающей точкой вместо 64-битных  для хранения параметров нейросети возможно без потери качества классификации, что соответсвует сжатию сети в 4 раза~\cite{Gupta2015floatingPoint}.

Метод предложенный в данной диссертации больше всего похож на низгоранговый подход~\cite{Denil2013predicting}, однако вместо ограничения матричного ранга мы рассматриваем матрицу слоя как многомерный линейный оператор и ограничиваем ее ТТ-ранг.



\section{Тензорный алгоритм сжатия} \label{sec:tensornet-tt-cnn}
В данном разделе описывается тензорный формат сжатия полносвязных слоев нейросети. Рассмотрим линейную часть функционального преобразования полносвязного слоя:
\begin{equation}
  \label{eq:tensornet-linear-layer}
  y = \mat{W} x + \vec{b},
\end{equation}
где $\mat{W}$ -- это матрица весов, а $\vec{b}$ -- вектор сдвига.
Рассмотрим как изменится преобразование~\ref{eq:tensornet-linear-layer} при использовании ТТ-формата для представления матрицы весов~$W$. Напомним, что ТТ-формат для матриц задается особым образом, заключающемся в построения вектор-индексов строк и столбцов ($i <-> (i_1, \ldots, i_d)$, $j <-> (j_1, \ldots, j_d)$) и построении ТТ-формата для тензора $\tens{W}$ получающегося путем перестановки и группировки индексов в исходной матрице (см.~\ref{??}).
Для удобства обозначений, рассмотрим так же тензорную версию вектора сдвига $\vec{b}$: $\tensel{B}(i_1, \ldots, j_d) = \vec{b}_j$
\begin{equation}
% \begin{aligned}
\label{eq:tensornet-layer-output-detailed}
\tensel{Y}(i_1, \ldots, i_d) =
%\sum_{\vec{j}} W (\vec{i}; \vec{j}) x_s(\vec{j}) + b(\vec{i}) = \\
\sum_{j_1, \ldots, j_d}  \!\!\mat{G}_1[i_1, j_1] \dots \mat{G}_d[i_d, j_d]\, \tensel{X}(j_1,\ldots,j_d) + \tensel{B}(i_1,\ldots,i_d).
% \end{aligned}
\end{equation}

Сложность вычисления выхода линейного слоя параметризованного матрицей в ТТ-формате~\eqref{eq:tensornet-layer-output-detailed} составляет $O(d r^2 m \max\{m, n\}^d) = O(d r^2 m \max\{M, N\})$~(см таблицу~\ref{tbl:complexity-comparison})



\subsection{Обучение модели}
В этом разделе мы выводим алгоритм подсчета градиентов произвольной функции потерь нейросети $L(w)$ по параметрам линейного слоя параметризованного матрицей заданной в ТТ-формате~\eqref{eq:tensornet-layer-output-detailed}. Параметры слоя -- это ядра ТТ-разложения $\tens{G}_1, \ldots, \tens{G}_d$ и тензор сдвига $\tens{B}$. Подсчет градиентов необходим для обучения нейросетей содержащих тензорно-параметризованные линейные слои.


To learn the parameters of the TT-representation of $\mat{W}$ one can use equation~\eqref{eq:traditional-gradient} to compute the gradient of the loss function w.r.t. the weight matrix $\mat{W}$, convert the gradient matrix into the TT-format (with the TT-SVD algorithm~\cite{oseledets2011ttMain}) and then add this gradient (multiplied by a step size) to the current estimate of the weight matrix: $\mat{W}_{k+1} = \mat{W}_{k} + \gamma_k \frac{\partial L}{\partial \mat{W}}$. However direct computation of $\frac{\partial L}{\partial \mat{W}}$ requires $O(MN)$ memory. Another concern is that the summation of matrices in the TT-format leads to the TT-ranks growth and the technique to control the increase of ranks and we would need to apply slow rounding procedure to control the growth of TT-ranks. A better way to learn the TensorNet parameters is to compute the gradient of the loss function directly w.r.t. the cores of the TT-representation of $\mat{W}$.


% If one choose to form an explicit vector after the TT-layer, the incoming gradient $\frac{\partial L}{y_s}$ vectors would be given exlicietly as well. In this case the computational complexity of this operation is $O(s r^2 m \max\{m, n\}^d D) = O(s r^2 m \max\{M, N\} D)$. If one choose to keep the result of the TT-layer in the TT-format, the incoming gradient $\frac{\partial L}{y_s}$ vectors would also be given in the TT-format. In this case the computational complexity would be $O()$.
In what follows we use shortened notation for prefix and postfix sequences of indices: $\vec{i}_k^- := (i_1, \dots, i_{k-1})$, $\vec{i}_k^+ := (i_{k+1}, \dots, i_d)$, $\vec{i} = (\vec{i}_k^-, i_k, \vec{i}_k^+)$. We also introduce a notation for partial core products:
\begin{equation}
\begin{aligned}
\vec{P}_k^-[\vec{i}_k^-, \vec{j}_k^-] &:= \vec{G}_1[i_1, j_1] \dots \vec{G}_{k-1}[i_{k-1}, j_{k-1}], \\
\vec{P}_k^+[\vec{i}_k^+, \vec{j}_k^+] &:= \vec{G}_{k+1}[i_{k+1}, j_{k+1}] \dots \vec{G}_d[i_d, j_d].
\end{aligned}
\end{equation}
Using this notation for any $k = 2, \ldots, d-1$ we can rewrite the definition of the TT-layer transformation~(\ref{eq:TT-layer-output-detailed}):
\begin{equation}
\label{eq:TT-layer-tensor-form}
\tensel{Y}(\vec{i}) = \tensel{Y}(\vec{i}_k^-, i_k, \vec{i}_k^+) =
\sum_{\vec{j}_k^-, j_k, \vec{j}_k^+}  \vec{P}_k^-[\vec{i}_k^-, \vec{j}_k^-] \mat{G}_k[i_k, j_k] \vec{P}_k^+[\vec{i}_k^+, \vec{j}_k^+] \tensel{X}(\vec{j}) + \tensel{B}(\vec{i}).
\end{equation}

The gradient of the loss function $L$ w.r.t. to the $k$-th core in the position $[\tilde{i}_k, \tilde{j}_k]$ can be computed using the chain rule:
\begin{equation}
\label{eq:d-L-d-G}
\underbrace{\frac{\partial{L}}{\partial{\vec{G}_k[\tilde{i}_k, \tilde{j}_k]}}}_{\rank_{k-1} \times \rank_{k}} = \sum_{\vec{i}} \frac{\partial{L}}{\partial{\tensel{Y}(\vec{i})}} \frac{\partial{\tensel{Y}}(\vec{i})}{\partial{\vec{G}_k[\tilde{i}_k, \tilde{j}_k]}}.
\end{equation}
Given the gradient matrices $\frac{\partial{\tensel{Y}}(\vec{i})}{\partial{\vec{G}_k[\tilde{i}_k, \tilde{j}_k]}}$ the summation~(\ref{eq:d-L-d-G}) can be done explicitly in $O(M)$ time, where $M$ is the length of the output vector $\vec{y}$. In what follows we show how to compute the matrix $\frac{\partial{\tensel{Y}}(\vec{i})}{\partial{\vec{G}_k[\tilde{i}_k, \tilde{j}_k]}}$ for any values of $\vec{i}$, $k$, $\tilde{i}_k$ and $\tilde{j}_k$.

\begin{table}\begin{center}
    \begin{tabular}{ l | l | l }
    %hline
    Operation & Time & Memory \rule{0pt}{1.0\normalbaselineskip} \\ \hline
    FC forward pass & $O(M N S)$ & $O(M N + N S + M S)$ \rule{0pt}{1.0\normalbaselineskip}\\ %\hline
    TT forward pass & $O(d r^2 m \max\{M, N\} S)$ & $O(r \max\{M, N\} S)$ \\ %\hline
    FC backward pass & $O(M N S)$ & $O(M N + N S + M S)$ \\ %\hline
    TT backward pass & $O(d^2 \rank^4 m \max\{M, N\} S)$ & $O(\rank^3 \max\{M, N\} S)$ \\[-0.5cm] %\hline
    \end{tabular}
    \end{center}
    \caption{Comparison of the asymptotic complexity and memory usage of a $M \times N$ TT-layer and a $M \times N$ fully-connected layer (FC) for an $S$-object batch. The input and the output tensor shapes are $m_1 \times \ldots \times m_d$ and $n_1 \times \ldots \times n_d$ respectively, $m$ is the maximal mode size $m = \max_{k = 1 \ldots d} m_k$ and $\rank$ is the maximal TT-rank. In principle the TT-operations can be implemented without linear memory w.r.t. $S$, however it will be less vectorized and cache efficient.\label{tbl:complexity-comparison}\vspace{-0.4cm}}
\end{table}


Let us fix the TT-core index $k \in \{1, \dots, d\}$ and $\tilde{i}_k \in \{1, \dots, m_k\}$, $\tilde{j}_k \in \{1, \dots, n_k\}$.
For any $\vec{i} = (i_1, \dots, i_d)$ such that $i_k \neq \tilde{i}_k$ the value of $\tensel{Y}(\vec{i})$ doesn't depend on the elements of $\mat{G}_k[\tilde{i}_k, \tilde{j}_k]$ making the corresponding gradient $\frac{\partial \tensel{Y}(\vec{i})}{\partial \mat{G}_k[\tilde{i}_k, \tilde{j}_k]}$ to be zero. Similarly, any summand in the eq.~\ref{eq:TT-layer-tensor-form} such that $j_k \neq \tilde{j}_k$ doesn't affect the gradient $\frac{\partial{\tensel{Y}}(\vec{i})}{\partial{\vec{G}_k[\tilde{i}_k, \tilde{j}_k]}}$. This observations allow us to consider only those $\vec{i}$ in eq.~\ref{eq:d-L-d-G}, where $i_k = \tilde{i}_k$ and only those $j_k$ in eq.~\ref{eq:TT-layer-tensor-form}, that are equal to $\tilde{j}_k$.

$\tensel{Y}(\vec{i}_k^-, \tilde{i}_k, \vec{i}_k^+)$ is a linear function of the core $\mat{G}_k[\tilde{i}_k, \tilde{j}_k]$~(see eq.~\ref{eq:TT-layer-tensor-form}) and its gradient is:
\begin{align*}
\frac{\partial{\tensel{Y}(\vec{i}_k^-, \tilde{i}_k, \vec{i}_k^+)}}{\partial{\vec{G}_k[\tilde{i}_k, \tilde{j}_k]}} = \sum_{\vec{j}_k^-, \vec{j}_k^+} \underbrace{\left ( \vec{P}_k^-[\vec{i}_k^-, \vec{j}_k^-] \right )^\intercal }_{\rank_{k-1} \times 1}  \underbrace{\left (\vec{P}_k^+[\vec{i}_k^+, \vec{j}_k^+] \right )^\intercal}_{1 \times \rank_{k}} \tensel{X}(\vec{j}_k^-, \tilde{j}_k, \vec{j}_k^+).
% &\frac{\partial{y_s}(\vec{i})}{\partial{\vec{G}_k[i_k, j_k]}} &&=\\
% &\sum_{\vec{j}_k^-, \vec{j}_k^+} &&\overbrace{G_1[i_1, j_1] \dots G_{k-1}[i_{k-1}, j_{k-1}]}^{\mathbb{R}^{1 \times \rank_{k-1}}}\\
% &&&\underbrace{G_{k+1}[i_{k+1}, j_{k+1}] \dots G_s[i_s, j_s]}_{\mathbb{R}^{\rank_{k} \times 1}} \underbrace{\vec{x}_s(\vec{j})}_{\mathbb{R}}
\end{align*}

Denote the partial sum as $\tens{R}_{k}$:
\begin{align*}
\tensel{R}_{k}(j_1, \dots, j_{k-1}, \tilde{j}_k, i_{k+1}, \dots, i_d, \alpha_{k}) = \tensel{R}_{k}(\vec{j}_{k}^-, \tilde{j}_k, \vec{i}_k^+, \alpha_{k}) = \sum_{\vec{j}_k^+} \vec{P}_k^+[\vec{i}_k^+, \vec{j}_k^+] ~ \tensel{X}(\vec{j}_k^-, \tilde{j}_k, \vec{j}_k^+).
\end{align*}
The column index $\alpha_{k+1}$ of the matrix $\mat{G}_{k+1}[i_{k+1}, j_{k+1}]$ is summed out because of the matrix product $\mat{G}_{k+1}[i_{k+1}, j_{k+1}] \cdot \mat{G}_{k+2}[i_{k+2}, j_{k+2}]$ and the row index $\alpha_{k}$ remains as the index of the partial sum $\tens{\tens{R}}_{k}$. The whole sequence of tensors $(\tens{\tens{R}}_{k})_{k = 1}^{d-1}$ can be computed via dynamic (by pushing sums w.r.t. each $j_{k+1}, \ldots, j_d$ inside the equation and summing out one index at a time) programming in $O(d r^2 m \max\{M, N\})$.

We have
\begin{align*}
\frac{\partial \tensel{Y}(\vec{i}_k^-, i_k, \vec{i}_k^+)}{\partial \mat{G}_k[\tilde{i}_k, \tilde{j}_k]} =
\begin{cases}
\sum_{\vec{j}_k^-} \vec{P}_k^-[\vec{i}_k^-, \vec{j}_k^-] \tensel{R}_{k}(\vec{j}_{k}^-, \tilde{j}_k, \vec{i}_k^+, \alpha_{k}) & \text{ if } i_k = \tilde{i}_k, \\[0.2cm]
0 & \text{ if } i_k \neq \tilde{i}_k.
\end{cases}
\end{align*}

Again, it can be efficiently computed via dynamic programming (by summing out one index $j_1, \ldots, j_{k-1}$ at a time). The overall computational complexity of the backward pass is $O(d^2 \rank^4 m \max\{M, N\})$.

The presented algorithm reduces to a sequence of matrix-by-matrix products and permutations of dimensions and thus can be accelerated on a GPU device.


Сложность $O(d r^2 n \max\{m, n\}^d) = O(d r^2 n \max\{M, N\})$.

\section{Альтернативная модель рекурентных нейронных сетей}
В данном разделе предлагается модель рекурентной нейросети параметризующаяся едниственным тензором в ТТ-формате. Так как множество тензоров фиксированного ТТ-ранга образует гладкое многообразие, такую модель становится возможным обучать с помощью римановой оптимизации, что позволяет ускорить обучение по сравнению с обычными методами оптимизации нейросетей такими как стохастический градиентный спуск.

\subsection{Предлагаемая модель}
Перед тем как ввести предлагамую модель в общем виде, рассмотрим частный случай когда объекты обучающей выборки описываются $3$ признаками. Предлагаемая модель представляет собой полином который включает по одному слагаемому для каждого подмножества признаков и в случае $3$-х мерных объектов выглядит следующим образом
\begin{equation}
\label{eq:polynomial-model-example}
\begin{aligned}
\widehat{y}(\vec{x}) &= \tensel{W}_{000} + \tensel{W}_{100} \,\, x_1 + \tensel{W}_{010} \,\, x_2 + \tensel{W}_{001} x_3 \\
&+ \tensel{W}_{110} \,\, x_1 x_2 + \tensel{W}_{101} \,\, x_1 x_3 + \tensel{W}_{011} \,\, x_2 x_3 \\
&+ \tensel{W}_{111} \,\, x_1 x_2 x_3.
\end{aligned}
\end{equation}
Обратите внимание, что все перестановки признаков в каждом слагаемом (например $x_1 x_2$ и $x_2 x_1$) соответсвуют лишь одному слагаемому с одним настраиваемым весом (например $\tensel{W}_{110}$).

В общем случае, подмножества признаков индексируются $d$-мерным бинарным вектором $(i_1, \ldots, i_d)$, где $i_k = 1$ тогда и только тогда когда $k$-ый признак принадлежит данному подмножеству. Используя данные обозначение, уравнение задающее модель в общем виде записывается следующим образом
\begin{equation}
\label{eq:polynomial-model}
\widehat{y}(\vec{x}) = \sum_{i_1=0}^1 \ldots \sum_{i_d=0}^1 \tensel{W}_{i_1 \ldots i_d} \prod_{k=1}^d x_k^{i_k}.
\end{equation}
В данных обозначениях подразумевается что $0^0 = 1$.
Таким образом, модель параметризуется $d$-мерным тензором~$\tens{W}$ состоящем из $2^d$ элементов.

Обратите внимание, что уравнение модели линейно относительно тензора параметров $\tens{W}$. Чтобы подчеркнуть этот факт уравнение~\eqref{eq:polynomial-model} можно переписать через скалярное произведение тензоров $\widehat{y}(\vec{x}) = \langle \tens{X}, \tens{W} \rangle$, где тензор $\tens{X}$ определяется следующим образом
\begin{equation}
\label{eq:X-definition}
\tensel{X}_{i_1 \ldots i_d} = \prod_{k=1}^d x_k^{i_k}.
\end{equation}
Обратите внимание, что в отличие от линейной модели~\eqref{eq:linear-model} в уравнение~\eqref{eq:polynomial-model} не требуется отдельный параметр сдвига $b$, так как сдвиг уже включен в модель в виде параметра~$\tensel{W}_{0 \ldots 0}$ (смотри пример уравнения модели~\eqref{eq:polynomial-model-example}).
Основная идея предлагаемой модели состоит в компактном представлении экспоненциально большого тензора параметров~$\tens{W}$ в ТТ-формате.

\subsection{Быстрый метод вывода}
В данном разделе показывается как вычислять уравнение модели~\eqref{eq:polynomial-model} за линейное число вычислительных операций от числа признаков $d$. Линейная сложность достигается благодаря ТТ-представлению тензора параметров~$\tens{W}$ и тензора объекта~$\tens{X}$~\eqref{eq:X-definition} в ТТ-формате с низкими рангами. 
Во время обучения тензор параметров~$\tens{W}$ обучается и инициализируется в ТТ-формате и его ТТ-ранг можно контролировать напрямую. В самом деле, следующие ТТ-ядра дают точное представление тензора~$\tens{X}$
\begin{equation*}
G_k[i_k] = x_k^{i_k} \in \mathbb{R}^{1 \times 1}, ~~ k=1, \ldots, d.
\end{equation*}
Так как $k$-ое ядро $G_k[i_k]$ представимо в виде матрицы размера $1 \times 1$ для любого значения индекса $i_k \in \{0, 1\}$, следовательно ТТ-ранг тензора~$\tens{X}$ равняется~$1$.

Как было показано в работе~\cite{oseledets2011ttMain}, скалярное произведение между двумя ТТ-тензорами с рангами $\rank$ и $1$ и размером каждой оси $2$ можно вычислить за время пропорциональное~$O(\rank^2 d)$. Покажем применение этого общего принципа для модели~\eqref{eq:polynomial-model}, так как в данном частном случае оно приводит к простым и элегентным формулам.

Перепишем уравнение модели~\eqref{eq:polynomial-model} предполагая что тензор $\tens{W}$ представлен в ТТ-формате.
\begin{equation*}
\begin{aligned}
\widehat{y}(\vec{x}) &= \sum_{i_1, \ldots, i_d} \tensel{W}_{i_1 \ldots i_d} \, \left ( \prod_{k=1}^d x_k^{i_k} \right )\\
&= \sum_{i_1, \ldots, i_d} G_1[i_1] \ldots G_d[i_d] \left ( \prod_{k=1}^d x_k^{i_k} \right ).
\end{aligned}
\end{equation*}

Сгруппируем множители зависящие от переменной $i_k$, $k=1, \ldots, d$
\begin{equation*}
%\label{eq:fast-model-equation}
\begin{aligned}
\widehat{y}(\vec{x}) &= \sum_{i_1, \ldots, i_d}  x_1^{i_1} G_1[i_1] \ldots x_d^{i_d} G_d[i_d]\\
&= \left ( \sum_{i_1=0}^1 x_1^{i_1} G_1[i_1] \right ) \ldots \left ( \sum_{i_d=0}^1 x_d^{i_d} G_d[i_d] \right )\\
&= \underbrace{A_1}_{1 \times \rank} \underbrace{A_2}_{\rank \times \rank} \ldots \underbrace{A_d}_{\rank \times 1},
\end{aligned}
\end{equation*}
где матрицы $A_k$ для $k=1, \ldots, d$ определены следующим образом
\begin{equation*}
A_k = \sum_{i_k=0}^1 x_k^{i_k} G_k[i_k] = G_k[0] + x_k G_k[1].
\end{equation*}
Итоговое значение $\widehat{y}(\vec{x})$ может быть вычислено с помощью $d-1$ матрично-векторных произведений и одного скалярного произведения векторов, что приводит к асимптотической сложности $O(r^2 d)$.

\subsection{Методы обучения}
Обучение модели~\eqref{eq:polynomial-model} соответсвует минимизации следующей (регуляризованной) функции потерь с ограничение на ТТ-ранг тензора параметров
\begin{equation}
\label{eq:TT-loss-minimization}
\begin{aligned}
& \underset{\tens{W}}{\text{minimize}}
& & L(\tens{W}), \\
& \text{subject to}
& & \ttrank(\tens{W}) = r_0,
\end{aligned}
\end{equation}
где функция потерь задана следующим образом
\begin{equation}
\label{eq:TT-loss}
L(\tens{W}) = \sum_{f=1}^N \ell\left(\langle \tens{X}^{(f)}, \tens{W} \rangle,\, y^{(f)}\right) + \frac{\lambda}{2} \norm{\tens{W}}^2_F.
\end{equation}

Рассмотрим два подхода к решению оптимизационной задачи~\eqref{eq:TT-loss-minimization}.
В качестве базового подхода, будем минимизировать функцию $L(\tens{W})$ стохастическим градиентным спуском примененнным к параметрам задающим ТТ-представление тензора $\tens{W}$ (элементам ТТ-ядер).

В качестве альтернативного подхода можно было бы рассмотреть (стохастический) градиентный спуск примененный к самому тензору~$\tens{W}$. В рамках такого подхода на каждой итерации следовало бы вычитать из градиент функции потерь из текущего значения $\tens{W}$. ТТ-формат дейтствительно позволяет вычитать тензоры, но это приводит к росту ТТ-рангов на каждой итерации, а операция ТТ-округления для контроля роста рангов имеет сложность кубическую по рангу. Как будет показано далее, ТТ-ранг градиента функции потерь пропорционален число объектов обучающей выборки рассматриваемых на данной итерации, и может достигать сотен, что делает такой подход непрактичным.

Для получения более эфективного метода оптимизации далее в этом разделе будет рассмотрен метод римановой оптимизации 
Экспериментально преимещуство римановой оптимизации над стохастическим граиентным спуском для данной задачи показано в разделе~\ref{sec:exp-riemannian-optimization}.

\subsubsection{риманова оптимизация\label{sec:exm-riemannian-optimization}}
Множество $d$-мерных тензоров фиксированного размера и с фиксированными ТТ-рангами $\rank$
\begin{equation*}
% \label{eq:TT-manifold}
\mathcal{M}_r = \{\tens{W} \in \mathbb{R}^{2 \times  \ldots \times 2}\!:\, \ttrank(\tens{W})=\rank\}
\end{equation*}
образует гладкое многообразие~\cite{holtz2012manifolds}.
Это свойство позволяет нам использоват аппарат римановой оптимизации для решение задачи~\eqref{eq:TT-loss-minimization}.
Метод риманового градиентного спуска состоит из следующих шагов повторяемых до сходимости (см Рис.~\ref{fig:riemannian-illustration}):
\begin{enumerate}
\item Спроектировать градиент $\frac{\partial L}{\partial \tens{W}}$ на касательное пространство $\mathcal{M}_r$ текущей точки $\tens{W}$. Данное касательное пространство будет обозначаться как $T_{\tens{W}} \mathcal{M}_r$, а операция проекции как $\tens{G} = P_{T_{\tens{W}} \mathcal{M}_r}(\frac{\partial L}{\partial \tens{W}})$.
\item Сделать шаг вдоль проекции градиента $\tens{G}$ с некоторым шагом~$\alpha$ (данная операция увеличивает ТТ-ранг).
\item Выполнить операцию ретракции чтобы приблизить полученную точку $\tens{W} - \alpha \tens{G}$  элементом многообразия $\mathcal{M}_r$, то-есть уменьшить ТТ-ранг обратно до значения $\rank$.
\end{enumerate}
Далее описано как реализовать каждый из озвученных выше шагов.

\begin{figure}[t]
% \vskip 0.2in
\begin{center}
  \resizebox{0.3\textwidth}{!}{
  \def\svgwidth{7cm}
  \normalsize
  \includesvg{images/manifold}
  }
  \caption{Иллюстрация одного шага риманового градиентного спуска. Шаг $\alpha$ положен в $1$ для наглядности иллюстрации. \label{fig:riemannian-illustration}}
  \end{center}
  % \vskip -0.2in
\end{figure}

В работе~\cite{lubich2015time} был предожен метод проектирования тензора~$\tens{Z}$ на касательное пространство многобразия~$\mathcal{M}_r$ в точке~$\tens{W}$ который состоит из следующих двух шагов: предобработки тензора~$\tens{W}$, которая может быть выполнена за $O(d \rank^3)$ арифметических операций; и проектирования тензора $\tens{Z}$, которая может быть выполнена за $O(d \rank^2 \ttrank(\tens{Z})^2)$ арифметических операций.
Так же было показано, что ТТ-ранг проекции ограничен константой не зависящей от ТТ-ранга проектируемого тензора~$\tens{Z}$:
\begin{equation*}
   \ttrank(P_{T_{\tens{W}} \mathcal{M}_r}(\tens{Z})) \leq 2 \ttrank(\tens{W}) = 2 \rank.
\end{equation*}

Рассмотрим градиент функции потерь~\eqref{eq:TT-loss}
\begin{equation}
\label{eq:TT-gradient}
\frac{\partial L}{\partial \tens{W}} = \sum_{f=1}^N \frac{\partial \ell}{\partial \widehat{y}} \tens{X}^{(f)} + \lambda \tens{W}.
\end{equation}

Пользуясь тем фактом, что любой ТТ-тензор принадлежит собственному касательному пространству $P_{T_{\tens{W}} \mathcal{M}_r}(\tens{W}) = \tens{W}$ и тем, что проекция это линейная операция, получаем
\begin{equation}
% \begin{aligned}
\label{eq:riemannian-gradient}
P_{T_{\tens{W}} \mathcal{M}_r}\left ( \frac{\partial L}{\partial \tens{W}} \right) = \sum_{f=1}^N \frac{\partial \ell}{\partial \widehat{y}} P_{T_{\tens{W}} \mathcal{M}_r}(\tens{X}^{(f)}) + \lambda \tens{W}.
% \end{aligned}
\end{equation}
Таким образом, проекция градиента является взвешанной суммой проекций тензоров $\tens{X}^{(f)}$, при этом все эти проекции могут быть выполнены параллельно.
Отметим так же что ТТ-ранг данных тензоров равне~$1$ (см разел~\ref{sec:inference}), поэтому вычислительная сложность подсчета $N$ проекций состовляет $O(d \rank^2 (\rank + N))$.
При этом ТТ-ранг проекции градиента  не превышает $2 \rank$ не зависимо от размера обучающей выборки~$N$.

Отметим так же, что для построения алгоритма вычисления проекции градиента использовался конкретный выбор члена отвещающего за регуляризацию. При выборе члена регуляризации отличного от  $L_2$ (например $L_1$), сложность вычисления проекции градиента может возрасти.

В качестве ретракции -- метода поиска аппроксимации данного тензора тензором с многообразия -- будет использоваться процедура ТТ-округления~\cite{oseledets2011ttMain}.

Чтобы метод стал применим к выборкам состоящим из большого числа объектов, будет применяться стохастическая версия риманового градиентного спуска: на каждой итерации выбирается случайное подмножество объектов обучающей выборки, вычисляется проекция градиента слагаемых функции потерь отвечающих выбранным объектам, делается шаг по направлению проекции градиента, и результат снова округляется до ранга $\rank$ (см. алгоритм~\ref{alg:rimeannian-optimization}).

Вычислительная сложность одной итерации стохастического риманового градиентного спуска для задачи~\eqref{eq:TT-loss-minimization}) сотавляет $O(dr^2(r + M))$ арифметических операций и состоит из $O(dr^2M)$ операций для подсчет скалярных произведений,  $O(dr^2(r + M))$ операций для вычисления проекции градиента и $O(dr^3)$ для ТТ-округлений, где $M$ это число объектов использующихся на каждой итерации метода.

\begin{algorithm}[t]
   \caption{Riemannian optimization}
   \label{alg:rimeannian-optimization}
\begin{algorithmic}
   \STATE {\bfseries Input:} Обучающая выборка $\{(\vec{x}^{(f)}, y^{(f)})\}_{f=1}^N$, требуемый ТТ-ранг $\rank_0$, число итераций~$T$, размер мини-батча~$M$, шаг обучения~$\alpha$, сила регуляризатора~$\lambda$
   \STATE {\bfseries Output:} $\tens{W}$ приближенно решающий задачу~\eqref{eq:TT-loss-minimization}
   \STATE Обучить линейную модель~\eqref{eq:linear-loss} и получить параметры $\vec{w}$ и $b$
   \STATE Инициализировать тензор $\tens{W}_0$ с ТТ-рангом равным $\rank_0$ используя $\vec{w}$ и $b$  (см. раздел~\ref{sec:exm-initialization})
   \FOR{$t := 1$ {\bfseries to} $T$}
        \STATE Случайно выбрать $M$ индексов $h_1, \ldots, h_M \sim \mathcal{U}(\{1, \ldots, N\})$
        \STATE $\tens{D}_t := \sum_{j=1}^M \frac{\partial \ell}{\partial \widehat{y}} \tens{X}^{(h_j)}  + \lambda \tens{W}_{t-1}$
        \STATE $\tens{G}_t := P_{T_{\tens{W}_{t-1}} \mathcal{M}_r}\left ( \tens{D}_t \right)$~\eqref{eq:riemannian-gradient}
        \STATE $\tens{W}_{t} := \ttround(\tens{W}_{t-1} - \alpha \tens{G}_t, \,r_0)$
   \ENDFOR
\end{algorithmic}
\end{algorithm}


\subsubsection{Initialization \label{sec:exm-initialization}}
We found that a random initialization for the TT-tensor $\tens{W}$ sometimes freezes the convergence of optimization method (see Sec.~\ref{sec:exp-initialization}).
We propose to initialize the optimization from the solution of the corresponding linear model~\eqref{eq:linear-model}.

The following theorem shows how to initialize the weight tensor $\tens{W}$ from a linear model.
\begin{theorem}
\label{thm:initialization-rank}
For any $d$-dimensional vector $\vec{w}$ and a bias term $b$ there exist a tensor $\tens{W}$ of TT-rank $2$, such that for any $d$-dimensional vector $\vec{x}$ and the corresponding object-tensor $\tens{X}$ the dot products $\langle \vec{x}, \vec{w} \rangle$ and $\langle \tens{X}, \tens{W} \rangle$ coincide.
\end{theorem}
For the proof of Theorem~\ref{thm:initialization-rank}, we refer the reader to Appendix~\ref{sec:app-initialization-rank-proof}.

%\begin{figure*}[t]
%  % \vskip 0.2in
%  \centering
%  \subfigure[Training set\hspace*{2.3cm}]{\includegraphics[width=0.45\textwidth]{images/riemannian_vs_plain_car_train.pdf}}
%  \hspace*{0.9cm}
%  \subfigure[Test set\hspace*{2.3cm}]{\includegraphics[width=0.45\textwidth]{images/riemannian_vs_plain_car_validation.pdf}}
%  \caption{Binarized Car dataset, a comparison between Riemannian optimization and SGD applied to the underlying parameters of the TT-format (the baseline) for the rank-$4$ Exponential Machines.
%  Numbers in the legend stand for the batch size.
%  The method marked with `rand init' in the legend (square markers) was initialized from a random TT-tensor (type-1 initialization, see Sec.~\ref{sec:exp-initialization} for details), all other methods were initialized from the solution of ordinary linear logistic regression.
%  \label{fig:riemannian_vs_plain_car}}
%  % \vskip -0.2in
%\end{figure*}

\subsection{Развтие модели \label{sec:exm-model-extension}}
In this section, we extend the proposed model to handle polynomials of any functions of the features.
As an example, consider the logarithms of the features in the $2$-dimensional case:
\begin{equation*}
  \begin{aligned}
    \widehat{y}^{\,\log}(\vec{x}) =& \,\tensel{W}_{00} + \tensel{W}_{01} x_1 +\tensel{W}_{10} x_2 + \tensel{W}_{11} x_1 x_2\\
    &+\tensel{W}_{20} \,\, \log(x_1) + \tensel{W}_{02} \,\, \log(x_2)\\
    &+ \tensel{W}_{12} \,\, x_1 \log(x_2)+ \tensel{W}_{21} \,\, x_2 \log(x_1)\\
    &+ \tensel{W}_{22} \,\, \log(x_1) \log(x_2).
  \end{aligned}
\end{equation*}
In the general case, to model interactions between $n_g$ functions $g_1, \ldots, g_{n_g}$ of the features we redefine the object-tensor as follows:
\begin{equation*}
\tensel{X}_{i_1 \ldots i_d} = \prod_{k=1}^d c(x_k, i_k),
\end{equation*}
where
\begin{equation*}
c(x_k, i_k) =
\begin{cases}
1, & \text{if } i_k = 0,\\
g_1(x_k), & \text{if } i_k = 1,\\
\ldots\\
g_{n_g}(x_k), & \text{if } i_k = n_g,\\
\end{cases}
\end{equation*}

The weight tensor $\tens{W}$ and the object-tensor $\tens{X}$ are now consist of $(n_g + 1)^d$ elements.
After this change to the object-tensor $\tens{X}$, learning and inference algorithms will stay unchanged compared to the original model~\eqref{eq:polynomial-model}.

\paragraph{Categorical features.} Our basic model handles categorical features $x_k \in \{1, \ldots, K\}$ by converting them into one-hot vectors $x_{k,1}, \ldots, x_{k,K}$. The downside of this approach is that it wastes the model capacity on modeling non-existing interactions between the one-hot vector elements $x_{k,1}, \ldots, x_{k,K}$ which correspond to the same categorical feature. Instead, we propose to use one TT-core per categorical feature and use the model extension technique with the following function
\begin{equation*}
c(x_k, i_k) =
\begin{cases}
1, & \text{if } x_k = i_k \text{ or } i_k = 0,\\
0, & \text{otherwise.}
\end{cases}
\end{equation*}
This allows us to cut the number of parameters per categorical feature from $2K\rank^2$ to $(K + 1)\rank^2$ without losing any representational power.

\section{Связь с рекурентными нейросетями}
In this section, we show the connection between the proposed model and Multiplicative Integration Recurrent Neural Networks~\cite{wu2016multiplicative}.

Recall the model equation, rewritten as a matrix multiplicaton~\eqref{eq:fast-model-equation}
\begin{equation}
\label{eq:fast-model-equation-rnn}
\widehat{y}(\vec{x}) = \left ( \sum_{i_1=0}^1 x_1^{i_1} G_1[i_1] \right ) \ldots \left ( \sum_{i_d=0}^1 x_d^{i_d} G_d[i_d] \right ).
\end{equation}

 Let's assume that instead of a $d$-dimensional object $\vec{x}$ we have a sequence of $d$ objects $x_1, \ldots, x_d$, and that each of the objects has two features $\vec{f}(x_k) = [1, x_k]$. Let's define $\vec{h}_k$ to be the product of the first $k$ matrices in Eq.~\eqref{eq:fast-model-equation-rnn}. Then, the model equation can be rewritten as
 \begin{align*}
\vec{h}_1 &=  \left ( \sum_{i_1=0}^1 f(x_1)_{i_1} G_1[i_1] \right ) = \vec{f}(x_1) \, G_1,\\
\vec{h}_k &= \vec{h}_{k-1} \left ( \sum_{i_k=0}^1 f(x_k)_{i_k} G_k[i_k] \right ) = \left (\vec{h}_{k-1} \otimes \vec{f}(x_k) \right) \, G_k,\\
\widehat{y}(\vec{x}) &= \vec{h}_d.
\end{align*}

Compare it to the formulas of Multiplicative Integration Recurrent Neural Networks assuming the identity function in place of a non-linearity $\phi$
% \begin{align*}
%\vec{h}_k = \mat{W}\vec{f}(x_k) \odot \mat{U} \vec{h}_{k-1} = \underbrace{\left (\mat{W} \mat{U} \right)}_{G_k} \left ( \vec{f}(x_k) \otimes \vec{h}_{k-1} \right )
%\end{align*}




\section{Выводы по главе} \label{sec:tensornet-conclusion}
Recent studies indicate high redundancy in current neural network parametrization. To exploit this redundancy we propose to use the TT-decomposition framework on the weight matrix of a fully-connected layer and to use the TT-cores as the parameters of the layer. This allows us to train fully-connected layers compressed by up to $200\,000\times$ compared to the explicit parametrization without significant error increase. Our experiments prove that is is possible to capture complex dependencies within the data by using much more compact representations of neural networks. On the other hand it becomes possible to use much wider layers than was available before and the preliminary experiments on the CIFAR-10 dataset show that wide and shallow TensorNets achieve promising results (setting new state-of-the-art for non-convolutional neural networks). Much more work needs to be done in this direction and careful combining of the TT-layer with recent advances in deep learning may lead to better results in different applied domains.

Another appealing property of the TT-layer is faster inference time (compared to the corresponding fully-connected layer). All in all a wide and shallow TensorNet can become a time and memory efficient model to use in real time applications and on mobile devices.

The main limiting factor for a $M \times N$ fully-connected layer size is its parameters number $MN$. The limiting factor for a $M \times N$ TT-layer is the maximal linear size $\max\{M, N\}$. As a future work we plan to consider the inputs and outputs of layers in TT-format thus allowing billions of hidden units in a TT-layer. Another direction of future work is to try TT-layers in different architectures such as Long Short-Term Memory~\cite{hochreiter1997LSTM}.

