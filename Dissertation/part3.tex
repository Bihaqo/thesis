\chapter{Нейронные сети} \label{chap:tensornet}
Искусственные нейронные сети опережают другие подходы во многих задачах распознавания образов, распознавания речи, перевода, и т.п. [CITE TODO]
Причиной успеха этого класса моделей является их большая гибкость и доступность большого количества вычислительных ресурсов (таких как кластеры графических ускорителей GPU).
И, хотя искусственные нейросети успешно применяются в промышленности (в машинном переводе, прототипах беспилотных автомобилей, умных камерах позволяющих получать снимки более высокого качества благодаря комбинированию нескольких кадров, и т.д.), их использования в мобильных и встроенных устройствах без доступа к интернету остается ограниченным.

Причина этому -- высокие требования к доступной памяти, вычислительной мощности и использованию аккумулятора. Например, популярная на сегодняшний день модель VGG-16 потребляет [TODO].

В связи с этим многие работы были посвящены задачам сжатия и ускорения нейронных сетей. Обратите внимание, что размер требуемой памяти является одним из ограничивающих факторов к снижению энергопотребления моделей. Так, при использовании 45 нм техпроцесса по технологии КМОП, требуется 0.9 пДж чтобы сложить два 32-битных числа с плавающей точкой, доступ к одному 32-битному числу в кэше SRAM требует 5 пДж, и доступ к 32-битному числу в оперативной памяти требует 640 пДж, что на три порядка больше по сравнению с операцией сложения. Большие нейросети не помещаются в кэш процессора и таким образом требует ресурсоемкого доступа к оперативной памяти.

В данной главе представлен тензорный подход к сжатию и ускорению (а значит и снижению энергопотребления) нейронных сетей, а так же модель рекурентной нейронной сети которую можно обучать при помощи римановой оптимизации, что позволяет добиться более высокой скорости обучения по сравнению с аналогами.

\section{Обзор существующих подходов к сжатию нейронных сетей} \label{sec:tensornet-alternatives}
В современных нейронных сетях до 99\% памяти занимают веса полносвязных слоев. В связи с этим, многие работы ставили своей целью исследование возможности сжатия полносвязных слоев. Одним из первых рассмотренных подходов к сжатию стало ограничение ранга матрицы полносвязного слоя~\cite{Denil2013predicting}. Ограничение случайного подмножества элементов матрицы так, чтобы все элементы каждого подмножества имели одно и то же значение~\cite{chen2015compressing} позволило сжимать полносвязные слои в 8 раз без потери качества. Так же рассматривалась более общая постановка, в которой ограничивалось число возможных значений которые могут принимать различные элементы матрицы~\cite{gong2014PQcompressing}. Было показано, что использование 16-битный чисел с плавающей точкой вместо 64-битных  для хранения параметров нейросети возможно без потери качества классификации, что соответсвует сжатию сети в 4 раза~\cite{Gupta2015floatingPoint}.

Метод предложенный в данной диссертации больше всего похож на низгоранговый подход~\cite{Denil2013predicting}, однако вместо ограничения матричного ранга матрицу слоя рассматривается как многомерный линейный оператор и ограничивается ее ТТ-ранг.



\section{Тензорный алгоритм сжатия} \label{sec:tensornet-tt-cnn}
В данном разделе описывается тензорный формат сжатия полносвязных слоев нейросети. Рассмотрим линейную часть функционального преобразования полносвязного слоя:
\begin{equation}
  \label{eq:tensornet-linear-layer}
  \vec{y} = \mat{W} x + \vec{b},
\end{equation}
где $\mat{W}$ -- это матрица весов, а $\vec{b}$ -- вектор сдвига.
Рассмотрим как изменится преобразование~\ref{eq:tensornet-linear-layer} при использовании ТТ-формата для представления матрицы весов~$\mat{W}$. Напомним, что ТТ-формат для матриц задается особым образом, заключающемся в построения вектор-индексов строк и столбцов ($i \leftrightarrow (i_1, \ldots, i_d)$, $j \leftrightarrow (j_1, \ldots, j_d)$) и построении ТТ-формата для тензора $\tens{W}$ получающегося путем перестановки и группировки индексов в исходной матрице (см. раздел~\ref{sec:tt-decomposition}).
Для удобства обозначений, рассмотрим так же тензорную версию вектора сдвига $\vec{b}$: $\tensel{B}_{i_1, \ldots, j_d} = b_j$
\begin{equation}
% \begin{aligned}
\label{eq:tensornet-layer-output-detailed}
\tensel{Y}_{i_1, \ldots, i_d} =
%\sum_{\vec{j}} W (\vec{i}; \vec{j}) x_s(\vec{j}) + b(\vec{i}) = \\
\sum_{j_1, \ldots, j_d}  \!\!\mat{G}_1[i_1, j_1] \dots \mat{G}_d[i_d, j_d]\, \tensel{X}_{j_1,\ldots,j_d} + \tensel{B}_{i_1,\ldots,i_d}.
% \end{aligned}
\end{equation}

Сложность вычисления выхода линейного слоя параметризованного матрицей в ТТ-формате~\eqref{eq:tensornet-layer-output-detailed} составляет $O(d r^2 m \max\{m, n\}^d) = O(d r^2 m \max\{M, N\})$~(см. таблицу~\ref{tbl:complexity-comparison}).


\begin{table}\begin{center}
    \begin{tabular}{ l | l | l }
    %hline
    Операция & Время & Память \rule{0pt}{1.0\normalbaselineskip} \\ \hline
    Полносвязный слой & $O(M N S)$ & $O(M N + N S + M S)$ \rule{0pt}{1.0\normalbaselineskip}\\ %\hline
    ТТ-слой & $O(d r^2 m \max\{M, N\} S)$ & $O(r \max\{M, N\} S)$ \\ %\hline
    \end{tabular}
    \end{center}
    \caption{Сравнение асимптотической сложности и требований по памяти перемножения обычной и ТТ-матрицы размера $M \times N$ на обучную матрицу размера $N \times S$. Тензорный размер входа и выхода матрицы равен $m_1 \times \ldots \times m_d$ и $n_1 \times \ldots \times n_d$ соответсвенно, $m$ обозначает максимальный размер по всем осям $m = \max_{k = 1 \ldots d} m_k$, а $\rank$ обозначает максимальный ТТ-ранг. \label{tbl:complexity-comparison}}
\end{table}


%Сложность $O(d r^2 n \max\{m, n\}^d) = O(d r^2 n \max\{M, N\})$.

\section{Альтернативная модель рекурентных нейронных сетей}
В данном разделе предлагается модель рекурентной нейросети параметризующаяся едниственным тензором в ТТ-формате. Так как множество тензоров фиксированного ТТ-ранга образует гладкое многообразие, такую модель становится возможным обучать с помощью римановой оптимизации, что позволяет ускорить обучение по сравнению с обычными методами оптимизации нейросетей такими как стохастический градиентный спуск.

\subsection{Предлагаемая модель}
Перед тем как ввести предлагамую модель в общем виде, рассмотрим частный случай когда объекты обучающей выборки описываются $3$ признаками. Предлагаемая модель представляет собой полином который включает по одному слагаемому для каждого подмножества признаков и в случае $3$-х мерных объектов выглядит следующим образом
\begin{equation}
\label{eq:polynomial-model-example}
\begin{aligned}
\widehat{y}(\vec{x}) &= \tensel{W}_{000} + \tensel{W}_{100} \,\, x_1 + \tensel{W}_{010} \,\, x_2 + \tensel{W}_{001} x_3 \\
&+ \tensel{W}_{110} \,\, x_1 x_2 + \tensel{W}_{101} \,\, x_1 x_3 + \tensel{W}_{011} \,\, x_2 x_3 \\
&+ \tensel{W}_{111} \,\, x_1 x_2 x_3.
\end{aligned}
\end{equation}
Обратите внимание, что все перестановки признаков в каждом слагаемом (например $x_1 x_2$ и $x_2 x_1$) соответсвуют лишь одному слагаемому с одним настраиваемым весом (например $\tensel{W}_{110}$).

В общем случае, подмножества признаков индексируются $d$-мерным бинарным вектором $(i_1, \ldots, i_d)$, где $i_k = 1$ тогда и только тогда когда $k$-ый признак принадлежит данному подмножеству. Используя данные обозначение, уравнение задающее модель в общем виде записывается следующим образом
\begin{equation}
\label{eq:polynomial-model}
\widehat{y}(\vec{x}) = \sum_{i_1=0}^1 \ldots \sum_{i_d=0}^1 \tensel{W}_{i_1 \ldots i_d} \prod_{k=1}^d x_k^{i_k}.
\end{equation}
В данных обозначениях подразумевается что $0^0 = 1$.
Таким образом, модель параметризуется $d$-мерным тензором~$\tens{W}$ состоящем из $2^d$ элементов.

Обратите внимание, что уравнение модели линейно относительно тензора параметров $\tens{W}$. Чтобы подчеркнуть этот факт уравнение~\eqref{eq:polynomial-model} можно переписать через скалярное произведение тензоров $\widehat{y}(\vec{x}) = \langle \tens{X}, \tens{W} \rangle$, где тензор $\tens{X}$ определяется следующим образом
\begin{equation}
\label{eq:X-definition}
\tensel{X}_{i_1 \ldots i_d} = \prod_{k=1}^d x_k^{i_k}.
\end{equation}
Обратите внимание, что в отличие от линейной модели~\eqref{eq:linear-model} в уравнение~\eqref{eq:polynomial-model} не требуется отдельный параметр сдвига $b$, так как сдвиг уже включен в модель в виде параметра~$\tensel{W}_{0 \ldots 0}$ (см. пример уравнения модели~\eqref{eq:polynomial-model-example}).
Основная идея предлагаемой модели состоит в компактном представлении экспоненциально большого тензора параметров~$\tens{W}$ в ТТ-формате.

\subsection{Быстрый метод вывода \label{sec:exm-inference}}
В данном разделе показывается как вычислять уравнение модели~\eqref{eq:polynomial-model} за линейное число вычислительных операций от числа признаков $d$. Линейная сложность достигается благодаря ТТ-представлению тензора параметров~$\tens{W}$ и тензора объекта~$\tens{X}$~\eqref{eq:X-definition} в ТТ-формате с низкими рангами. 
Во время обучения тензор параметров~$\tens{W}$ инициализируются и обучается в ТТ-формате и его ТТ-ранг можно контролировать напрямую. В самом деле, следующие ТТ-ядра дают точное представление тензора~$\tens{X}$
\begin{equation*}
G_k[i_k] = x_k^{i_k} \in \mathbb{R}^{1 \times 1}, ~~ k=1, \ldots, d.
\end{equation*}
Так как $k$-ое ядро $G_k[i_k]$ представимо в виде матрицы размера $1 \times 1$ для любого значения индекса $i_k \in \{0, 1\}$, следовательно ТТ-ранг тензора~$\tens{X}$ равняется~$1$.

Как было показано в работе~\cite{oseledets2011ttMain}, скалярное произведение между двумя ТТ-тензорами с рангами $\rank$ и $1$ и размером каждой оси $2$ можно вычислить за время пропорциональное~$O(\rank^2 d)$. Покажем применение этого общего принципа для модели~\eqref{eq:polynomial-model}, так как в данном частном случае оно приводит к простым и элегентным формулам.

Перепишем уравнение модели~\eqref{eq:polynomial-model} предполагая что тензор $\tens{W}$ представлен в ТТ-формате.
\begin{equation*}
\begin{aligned}
\widehat{y}(\vec{x}) &= \sum_{i_1, \ldots, i_d} \tensel{W}_{i_1 \ldots i_d} \, \left ( \prod_{k=1}^d x_k^{i_k} \right )\\
&= \sum_{i_1, \ldots, i_d} G_1[i_1] \ldots G_d[i_d] \left ( \prod_{k=1}^d x_k^{i_k} \right ).
\end{aligned}
\end{equation*}

Сгруппируем множители зависящие от переменной $i_k$, $k=1, \ldots, d$
\begin{equation*}
%\label{eq:fast-model-equation}
\begin{aligned}
\widehat{y}(\vec{x}) &= \sum_{i_1, \ldots, i_d}  x_1^{i_1} G_1[i_1] \ldots x_d^{i_d} G_d[i_d]\\
&= \left ( \sum_{i_1=0}^1 x_1^{i_1} G_1[i_1] \right ) \ldots \left ( \sum_{i_d=0}^1 x_d^{i_d} G_d[i_d] \right )\\
&= \underbrace{A_1}_{1 \times \rank} \underbrace{A_2}_{\rank \times \rank} \ldots \underbrace{A_d}_{\rank \times 1},
\end{aligned}
\end{equation*}
где матрицы $A_k$ для $k=1, \ldots, d$ определены следующим образом
\begin{equation*}
A_k = \sum_{i_k=0}^1 x_k^{i_k} G_k[i_k] = G_k[0] + x_k G_k[1].
\end{equation*}
Итоговое значение $\widehat{y}(\vec{x})$ может быть вычислено с помощью $d-1$ матрично-векторных произведений и одного скалярного произведения векторов, что приводит к асимптотической сложности $O(r^2 d)$.

\subsection{Методы обучения}
Обучение модели~\eqref{eq:polynomial-model} соответсвует минимизации следующей (регуляризованной) функции потерь с ограничение на ТТ-ранг тензора параметров
\begin{equation}
\label{eq:TT-loss-minimization}
\begin{aligned}
& \underset{\tens{W}}{\text{minimize}}
& & L(\tens{W}), \\
& \text{subject to}
& & \ttrank(\tens{W}) = r_0,
\end{aligned}
\end{equation}
где функция потерь задана следующим образом
\begin{equation}
\label{eq:TT-loss}
L(\tens{W}) = \sum_{f=1}^N \ell\left(\langle \tens{X}^{(f)}, \tens{W} \rangle,\, y^{(f)}\right) + \frac{\lambda}{2} \norm{\tens{W}}^2_F.
\end{equation}

Рассмотрим два подхода к решению оптимизационной задачи~\eqref{eq:TT-loss-minimization}.
В качестве базового подхода, будем минимизировать функцию $L(\tens{W})$ стохастическим градиентным спуском примененнным к параметрам задающим ТТ-представление тензора $\tens{W}$ (элементам ТТ-ядер).

В качестве альтернативного подхода можно было бы рассмотреть (стохастический) градиентный спуск примененный к самому тензору~$\tens{W}$. В рамках такого подхода на каждой итерации следовало бы вычитать градиент функции потерь из текущего значения тензора~$\tens{W}$. ТТ-формат действительно позволяет вычитать тензоры, но это приводит к росту ТТ-рангов на каждой итерации, а операция ТТ-округления для контроля роста рангов имеет сложность кубическую по рангу. Как будет показано далее, ТТ-ранг градиента функции потерь пропорционален числу объектов обучающей выборки рассматриваемых на данной итерации, и может достигать сотен, что делает такой подход непрактичным.

Для получения более эффективного метода оптимизации далее в этом разделе будет рассмотрен метод римановой оптимизации.
Экспериментально преимущество римановой оптимизации над стохастическим градиентным спуском для данной задачи показано в разделе~\ref{sec:exp-riemannian-optimization}.

\subsubsection{Справка по методам римановой оптимизации \label{sec:riemannian-help}}
Данный раздел является обзорным, и в нем приведена основная информация по методам римановой оптимизации используемая далее.

Ключевым понятием в римановой оптимизации является \emph{гладкое многообразие}. Пусть $\mathcal{U} \subset \mathbb{R}^n$ и $\mathcal{V} \subset \mathbb{R}^m$ это открытые множества. Отображение $f: \mathcal{U} \to \mathcal{V}$ называется гладким если $f$ бесконечно непрерывно диффиренцируемый. Биективное отображение $f$ называют \emph{диффеоморфизмом} если и $f$ и обратное отображение $f^{-1}$ являются гладкими. Множества $\mathcal{U}$ и $\mathcal{V}$ в таком случае называют \emph{диффеоморфными}.

Пусть $\mathcal{M} \subset \mathbb{R}^n$ это некоторая поверхность в $\mathbb{R}^n$. Основная идея введения понятия гладкого многообразия состоит в том, чтобы построить соответствие между участками поверхности $\mathcal{M}$ и евклидовом множеством $\mathbb{R}^m$ чтобы свести операции над $\mathcal{M}$ к операциям над $\mathbb{R}^m$. Данная идея иллюстрируется на Рис.~\ref{fig:manifold-def}, а формальное определение дается ниже:
\begin{definition}
	\cite{RS13, Def. 1.3} Множество $\mathcal{M} \subset \mathbb{R}^n$ называют гладким подмногообразием $\mathbb{R}^n$ если для любого $x \in \mathcal{M}$	 найдется открытая окрестность $\mathcal{U} \subset \mathbb{R}^n$ такая, что $\mathcal{U} \cap \mathcal{M}$ диффеоморфно открытому подмножеству $\mathcal{V} \subset \mathbb{R}^m$. Диффеоморфизм $\phi: \mathcal{U} \cap \mathcal{M} \to \mathcal{V}$ называют атласом многообразия $\mathcal{M}$.
\end{definition}

\begin{figure}[t]
% \vskip 0.2in
\begin{center}
  \includegraphics[width=0.8\textwidth]{images/manifold_def.png}
  \caption{\alert{TODO: перерисовать} Иллюстрация гладкого $m$-мерного многообразия в $\mathbb{R}^n$. Атлас $\phi$ отображает окрестность  $\mathcal{U} \cap \mathcal{M}$ точки $x \in \mathcal{M}$ в открытое подмножество $\mathbb{R}^n$. \label{fig:manifold-def}}
  \end{center}
  % \vskip -0.2in
\end{figure}

В работе~\cite{holtz2012manifolds} было показано, что множество $d$-мерных тензоров фиксированного размера и с фиксированными ТТ-рангами $\rank$
\begin{equation*}
% \label{eq:TT-manifold}
\mathcal{M}_r = \{\tens{W} \in \mathbb{R}^{n_1 \times  \ldots \times n_d}\!:\, \ttrank(\tens{W})=\rank\}
\end{equation*}
образует гладкое многообразие.

В каждой точке $x \in \mathcal{M}$ гладкого многообразия $\mathcal{M}$ можно построить \emph{касательное пространство}, интуитивно которое можно представлять как плоскость ортогональную поверхности $\mathcal{M}$ в данной точке (см. Рис.~\ref{fig:tangent-space}). Формально, касательное пространство определяется следующим образом
\begin{definition}
	\cite{RS13, Def. 1.21} Пусть $\mathcal{M} \subset \mathbb{R}^n$ это гладкое $m$-мерное подмногообразие. Вектор $\xi \in \mathbb{R}^n$ называют касательным вектором многообразия $\mathcal{M}$ в точке $x \in \mathcal{M}$ если найдется гладкая кривая $\gamma: \mathbb{R} \to \mathcal{M}$ такая что
\[
\gamma(0) = x, ~~ \gamma'(0) = \lim_{t \to 0} \frac{\gamma(t) - \gamma(0)}{t} = \xi.
\]
Множество касательных векторов $\mathcal{M}$ в точке $x$,
\[
T_x \mathcal{M} := \{\gamma'(0) \mid \gamma : \mathbb{R} \to \mathcal{M} \text{ является гладким}, \gamma(0) = x\},
\]
называется касательным пространством $\mathcal{M}$ в точке $x$.
\end{definition}

\begin{figure}[t]
\begin{center}
  \includegraphics[width=0.8\textwidth]{images/tangent_space.png}
  \caption{\alert{TODO: перерисовать} Иллюстрация касательного пространства $T_x \mathcal{M}$ подмногообразия $\mathcal{M} \subset \mathbb{R}^n$ в точке $x \in \mathcal{M}$. Два различных вектора $\gamma_1'(0)$ и $\gamma_2'(0)$ реализуются двумя различными кривыми $\gamma_1$ и $\gamma_2$ на многообразие. \label{fig:tangent-space}}
  \end{center}
\end{figure}


Как показано в работе~\cite{Steinlechner2016}, касательное пространство $T_{\tens{W}} \mathcal{M}_{\rank}$ многообразия тензоров фиксированного ТТ ранга $\mathcal{M}_r$ в точке $\tens{W} \in \mathcal{M}_r$ можно определить как множество тензоров $\tens{Y}$ ТТ-ранга не превосходящего $2 \rank$ определяемых следующими ТТ-ядрами для $k = 2, \ldots, d-1$
\[
G_1^{\tens{Y}}[i_1] = \begin{bmatrix}
\delta \widetilde{U}_1[i_1] & U_1[i_1]
\end{bmatrix}, ~~ G_k^{\tens{Y}}[i_k] = \begin{bmatrix}
V_k[i_k] & 0\\
\delta \widetilde{U}_k[i_k] & U_k[i_k]\\
\end{bmatrix}, ~~ G_d^{\tens{Y}}[i_d] = \begin{bmatrix}
V_d[i_d]\\
\delta \widetilde{U}_d[i_d]\\
\end{bmatrix},
\]
где матрицы $U_k[i_k]$ и $V_k[i_k]$, $k = 1, \ldots, d$ это результат ортогонализации ядра $G^{\tens{W}}_k[i_k]$ слева-на-право и справо-на-лево соответственно (детали по поводу правой и левой ортогонализации см. в работе~\cite{oseledets2011ttMain}), а матрицы $\delta \widetilde{U}_k[i_k]$ это произвольные матрицы размера $\rank_{k-1} \times \rank_k$.


Можно построить такой алгоритм построения проекции:

\subsubsection{Риманова оптимизация для задачи~\eqref{eq:TT-loss-minimization} \label{sec:exm-riemannian-optimization}}

Как было отмечено выше, множество тензоров фиксированного ТТ-ранга образует гладкое многообразие, что делает возможным применение аппарата римановой оптимизации для решения задачи~\eqref{eq:TT-loss-minimization}.
Метод риманового градиентного спуска состоит из следующих шагов повторяемых до сходимости (см. Рис.~\ref{fig:riemannian-illustration}):
\begin{enumerate}
\item Спроектировать градиент $\frac{\partial L}{\partial \tens{W}}$ на касательное пространство $\mathcal{M}_r$ текущей точки $\tens{W}$. Данное касательное пространство будет обозначаться как $T_{\tens{W}} \mathcal{M}_r$, а операция проекции как $\tens{G} = P_{T_{\tens{W}} \mathcal{M}_r}(\frac{\partial L}{\partial \tens{W}})$.
\item Сделать шаг вдоль проекции градиента $\tens{G}$ с некоторым шагом~$\alpha$ (данная операция увеличивает ТТ-ранг).
\item Выполнить операцию ретракции чтобы приблизить полученную точку $\tens{W} - \alpha \tens{G}$  элементом многообразия $\mathcal{M}_r$, то-есть уменьшить ТТ-ранг обратно до значения $\rank$.
\end{enumerate}
Далее описано как реализовать каждый из перечисленных выше шагов.

\begin{figure}[t]
% \vskip 0.2in
\begin{center}
  \resizebox{0.5\textwidth}{!}{
  \def\svgwidth{7cm}
  \normalsize
  \includesvg{images/manifold}
  }
  \caption{Иллюстрация одного шага риманового градиентного спуска. Шаг $\alpha$ положен в $1$ для наглядности иллюстрации. \label{fig:riemannian-illustration}}
  \end{center}
  % \vskip -0.2in
\end{figure}

В работе~\cite{lubich2015time} был предложен метод проектирования тензора~$\tens{Z}$ на касательное пространство многообразия~$\mathcal{M}_r$ в точке~$\tens{W}$ который состоит из двух шагов: предобработки тензора~$\tens{W}$, которая может быть выполнена за $O(d \rank^3)$ арифметических операций; и проектирования тензора $\tens{Z}$, которая может быть выполнена за $O(d \rank^2 \ttrank(\tens{Z})^2)$ арифметических операций.
Так же было показано, что ТТ-ранг проекции ограничен константой не зависящей от ТТ-ранга проектируемого тензора~$\tens{Z}$:
\begin{equation*}
   \ttrank(P_{T_{\tens{W}} \mathcal{M}_r}(\tens{Z})) \leq 2 \ttrank(\tens{W}) = 2 \rank.
\end{equation*}

Рассмотрим градиент функции потерь~\eqref{eq:TT-loss}
\begin{equation}
\label{eq:TT-gradient}
\frac{\partial L}{\partial \tens{W}} = \sum_{f=1}^N \frac{\partial \ell}{\partial \widehat{y}} \tens{X}^{(f)} + \lambda \tens{W}.
\end{equation}

Пользуясь тем фактом, что любой ТТ-тензор принадлежит собственному касательному пространству $P_{T_{\tens{W}} \mathcal{M}_r}(\tens{W}) = \tens{W}$ и тем, что проекция это линейная операция, получаем
\begin{equation}
% \begin{aligned}
\label{eq:riemannian-gradient}
P_{T_{\tens{W}} \mathcal{M}_r}\left ( \frac{\partial L}{\partial \tens{W}} \right) = \sum_{f=1}^N \frac{\partial \ell}{\partial \widehat{y}} P_{T_{\tens{W}} \mathcal{M}_r}(\tens{X}^{(f)}) + \lambda \tens{W}.
% \end{aligned}
\end{equation}
Таким образом, проекция градиента является взвешенной суммой проекций тензоров $\tens{X}^{(f)}$, при этом все эти проекции могут быть выполнены параллельно.
Отметим так же что ТТ-ранг данных тензоров равне~$1$ (см. раздел~\ref{sec:exm-inference}), поэтому вычислительная сложность подсчета $N$ проекций составляет $O(d \rank^2 (\rank + N))$.
При этом ТТ-ранг проекции градиента  не превышает $2 \rank$ не зависимо от размера обучающей выборки~$N$.

Отметим так же, что для построения алгоритма вычисления проекции градиента использовался конкретный выбор члена отвещающего за регуляризацию. При выборе члена регуляризации отличного от  $L_2$ (например $L_1$), сложность вычисления проекции градиента может возрасти.

В качестве ретракции -- метода поиска аппроксимации данного тензора тензором с многообразия -- будет использоваться процедура ТТ-округления~\cite{oseledets2011ttMain}.

Чтобы метод стал применим к выборкам состоящим из большого числа объектов, будет применяться стохастическая версия риманового градиентного спуска: на каждой итерации выбирается случайное подмножество объектов обучающей выборки, вычисляется проекция градиента слагаемых функции потерь отвечающих выбранным объектам, делается шаг по направлению проекции градиента, и результат снова округляется до ранга $\rank$ (см. алгоритм~\ref{alg:rimeannian-optimization}).

Вычислительная сложность одной итерации стохастического риманового градиентного спуска для задачи~\eqref{eq:TT-loss-minimization}) сотавляет $O(dr^2(r + M))$ арифметических операций и состоит из $O(dr^2M)$ операций для подсчет скалярных произведений,  $O(dr^2(r + M))$ операций для вычисления проекции градиента и $O(dr^3)$ для ТТ-округлений, где $M$ это число объектов использующихся на каждой итерации метода.


\begin{algorithm}[t]
   \caption{Риманов градиентный спуск для задачи~\eqref{eq:TT-loss-minimization}}
   \label{alg:rimeannian-optimization}
\begin{algorithmic}
   \REQUIRE Обучающая выборка $\{(\vec{x}^{(f)}, y^{(f)})\}_{f=1}^N$, требуемый ТТ-ранг $\rank_0$, число итераций~$T$, размер мини-батча~$M$, шаг обучения~$\alpha$, сила регуляризатора~$\lambda$
   \ENSURE $\tens{W}$ приближенно решающий задачу~\eqref{eq:TT-loss-minimization}
   \STATE Обучить линейную модель~\eqref{eq:linear-loss} и получить параметры $\vec{w}$ и $b$
   \STATE Инициализировать тензор $\tens{W}_0$ с ТТ-рангом равным $\rank_0$ используя $\vec{w}$ и $b$  (см. раздел~\ref{sec:exm-initialization})
   \FOR{$t := 1$ {\bfseries to} $T$}
        \STATE Случайно выбрать $M$ индексов $h_1, \ldots, h_M \sim \mathcal{U}(\{1, \ldots, N\})$
        \STATE $\tens{D}_t := \sum_{j=1}^M \frac{\partial \ell}{\partial \widehat{y}} \tens{X}^{(h_j)}  + \lambda \tens{W}_{t-1}$
        \STATE $\tens{G}_t := P_{T_{\tens{W}_{t-1}} \mathcal{M}_r}\left ( \tens{D}_t \right)$~\eqref{eq:riemannian-gradient}
        \STATE $\tens{W}_{t} := \ttround(\tens{W}_{t-1} - \alpha \tens{G}_t, \,r_0)$
   \ENDFOR
\end{algorithmic}
\end{algorithm}


\subsubsection{Initialization \label{sec:exm-initialization}}
We found that a random initialization for the TT-tensor $\tens{W}$ sometimes freezes the convergence of optimization method (see Sec.~\ref{sec:exp-initialization}).
We propose to initialize the optimization from the solution of the corresponding linear model~\eqref{eq:linear-model}.

The following theorem shows how to initialize the weight tensor $\tens{W}$ from a linear model.
\begin{theorem}
\label{thm:initialization-rank}
For any $d$-dimensional vector $\vec{w}$ and a bias term $b$ there exist a tensor $\tens{W}$ of TT-rank $2$, such that for any $d$-dimensional vector $\vec{x}$ and the corresponding object-tensor $\tens{X}$ the dot products $\langle \vec{x}, \vec{w} \rangle$ and $\langle \tens{X}, \tens{W} \rangle$ coincide.
\end{theorem}
For the proof of Theorem~\ref{thm:initialization-rank}, we refer the reader to Appendix~\ref{sec:app-initialization-rank-proof}.

%\begin{figure*}[t]
%  % \vskip 0.2in
%  \centering
%  \subfigure[Training set\hspace*{2.3cm}]{\includegraphics[width=0.45\textwidth]{images/riemannian_vs_plain_car_train.pdf}}
%  \hspace*{0.9cm}
%  \subfigure[Test set\hspace*{2.3cm}]{\includegraphics[width=0.45\textwidth]{images/riemannian_vs_plain_car_validation.pdf}}
%  \caption{Binarized Car dataset, a comparison between Riemannian optimization and SGD applied to the underlying parameters of the TT-format (the baseline) for the rank-$4$ Exponential Machines.
%  Numbers in the legend stand for the batch size.
%  The method marked with `rand init' in the legend (square markers) was initialized from a random TT-tensor (type-1 initialization, see Sec.~\ref{sec:exp-initialization} for details), all other methods were initialized from the solution of ordinary linear logistic regression.
%  \label{fig:riemannian_vs_plain_car}}
%  % \vskip -0.2in
%\end{figure*}

\subsection{Развтие модели \label{sec:exm-model-extension}}
In this section, we extend the proposed model to handle polynomials of any functions of the features.
As an example, consider the logarithms of the features in the $2$-dimensional case:
\begin{equation*}
  \begin{aligned}
    \widehat{y}^{\,\log}(\vec{x}) =& \,\tensel{W}_{00} + \tensel{W}_{01} x_1 +\tensel{W}_{10} x_2 + \tensel{W}_{11} x_1 x_2\\
    &+\tensel{W}_{20} \,\, \log(x_1) + \tensel{W}_{02} \,\, \log(x_2)\\
    &+ \tensel{W}_{12} \,\, x_1 \log(x_2)+ \tensel{W}_{21} \,\, x_2 \log(x_1)\\
    &+ \tensel{W}_{22} \,\, \log(x_1) \log(x_2).
  \end{aligned}
\end{equation*}
In the general case, to model interactions between $n_g$ functions $g_1, \ldots, g_{n_g}$ of the features we redefine the object-tensor as follows:
\begin{equation*}
\tensel{X}_{i_1 \ldots i_d} = \prod_{k=1}^d c(x_k, i_k),
\end{equation*}
where
\begin{equation*}
c(x_k, i_k) =
\begin{cases}
1, & \text{if } i_k = 0,\\
g_1(x_k), & \text{if } i_k = 1,\\
\ldots\\
g_{n_g}(x_k), & \text{if } i_k = n_g,\\
\end{cases}
\end{equation*}

The weight tensor $\tens{W}$ and the object-tensor $\tens{X}$ are now consist of $(n_g + 1)^d$ elements.
After this change to the object-tensor $\tens{X}$, learning and inference algorithms will stay unchanged compared to the original model~\eqref{eq:polynomial-model}.

\paragraph{Categorical features.} Our basic model handles categorical features $x_k \in \{1, \ldots, K\}$ by converting them into one-hot vectors $x_{k,1}, \ldots, x_{k,K}$. The downside of this approach is that it wastes the model capacity on modeling non-existing interactions between the one-hot vector elements $x_{k,1}, \ldots, x_{k,K}$ which correspond to the same categorical feature. Instead, we propose to use one TT-core per categorical feature and use the model extension technique with the following function
\begin{equation*}
c(x_k, i_k) =
\begin{cases}
1, & \text{if } x_k = i_k \text{ or } i_k = 0,\\
0, & \text{otherwise.}
\end{cases}
\end{equation*}
This allows us to cut the number of parameters per categorical feature from $2K\rank^2$ to $(K + 1)\rank^2$ without losing any representational power.

\section{Связь с рекурентными нейросетями}
In this section, we show the connection between the proposed model and Multiplicative Integration Recurrent Neural Networks~\cite{wu2016multiplicative}.

Recall the model equation, rewritten as a matrix multiplicaton~\eqref{eq:fast-model-equation}
\begin{equation}
\label{eq:fast-model-equation-rnn}
\widehat{y}(\vec{x}) = \left ( \sum_{i_1=0}^1 x_1^{i_1} G_1[i_1] \right ) \ldots \left ( \sum_{i_d=0}^1 x_d^{i_d} G_d[i_d] \right ).
\end{equation}

 Let's assume that instead of a $d$-dimensional object $\vec{x}$ we have a sequence of $d$ objects $x_1, \ldots, x_d$, and that each of the objects has two features $\vec{f}(x_k) = [1, x_k]$. Let's define $\vec{h}_k$ to be the product of the first $k$ matrices in Eq.~\eqref{eq:fast-model-equation-rnn}. Then, the model equation can be rewritten as
 \begin{align*}
\vec{h}_1 &=  \left ( \sum_{i_1=0}^1 f(x_1)_{i_1} G_1[i_1] \right ) = \vec{f}(x_1) \, G_1,\\
\vec{h}_k &= \vec{h}_{k-1} \left ( \sum_{i_k=0}^1 f(x_k)_{i_k} G_k[i_k] \right ) = \left (\vec{h}_{k-1} \otimes \vec{f}(x_k) \right) \, G_k,\\
\widehat{y}(\vec{x}) &= \vec{h}_d.
\end{align*}

Compare it to the formulas of Multiplicative Integration Recurrent Neural Networks assuming the identity function in place of a non-linearity $\phi$
% \begin{align*}
%\vec{h}_k = \mat{W}\vec{f}(x_k) \odot \mat{U} \vec{h}_{k-1} = \underbrace{\left (\mat{W} \mat{U} \right)}_{G_k} \left ( \vec{f}(x_k) \otimes \vec{h}_{k-1} \right )
%\end{align*}




\section{Выводы по главе} \label{sec:tensornet-conclusion}
Recent studies indicate high redundancy in current neural network parametrization. To exploit this redundancy we propose to use the TT-decomposition framework on the weight matrix of a fully-connected layer and to use the TT-cores as the parameters of the layer. This allows us to train fully-connected layers compressed by up to $200\,000\times$ compared to the explicit parametrization without significant error increase. Our experiments prove that is is possible to capture complex dependencies within the data by using much more compact representations of neural networks. On the other hand it becomes possible to use much wider layers than was available before and the preliminary experiments on the CIFAR-10 dataset show that wide and shallow TensorNets achieve promising results (setting new state-of-the-art for non-convolutional neural networks). Much more work needs to be done in this direction and careful combining of the TT-layer with recent advances in deep learning may lead to better results in different applied domains.

Another appealing property of the TT-layer is faster inference time (compared to the corresponding fully-connected layer). All in all a wide and shallow TensorNet can become a time and memory efficient model to use in real time applications and on mobile devices.

The main limiting factor for a $M \times N$ fully-connected layer size is its parameters number $MN$. The limiting factor for a $M \times N$ TT-layer is the maximal linear size $\max\{M, N\}$. As a future work we plan to consider the inputs and outputs of layers in TT-format thus allowing billions of hidden units in a TT-layer. Another direction of future work is to try TT-layers in different architectures such as Long Short-Term Memory~\cite{hochreiter1997LSTM}.

